{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_file.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNmIHJ3MsjMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#########################################################\n",
        "# Decryption of Enigma coded strings\n",
        "#########################################################\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Part of coding challenge for Scale AI ML Role\n",
        "# Github link: https://github.com/hkhoont/scale_ai_engima_machine\n",
        "# email: hsk2147@columbia.edu\n",
        "# -----------------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9nfWmZ8KTqr",
        "colab_type": "code",
        "outputId": "0f93bfcd-a04b-476a-d691-af48b1e16e7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Mounting the drive of User testing this script\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root='/content/drive/My Drive/Applications/# Scale AI/runs/'"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sPnmX1K8BoF",
        "colab_type": "text"
      },
      "source": [
        "## A - Installing  and importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8amUMvWrGKi",
        "colab_type": "code",
        "outputId": "41ae335b-e0a6-4e56-b02f-7a4897ccab1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "!pip install py-enigma\n",
        "!pip install faker\n",
        "!pip install tensorboardX==1.4\n",
        "!pip install tqdm\n",
        "!pip install hide_code\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: py-enigma in /usr/local/lib/python3.6/dist-packages (0.1)\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from faker) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.6/dist-packages (from faker) (2.6.1)\n",
            "Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.6/dist-packages (from faker) (1.3)\n",
            "Requirement already satisfied: tensorboardX==1.4 in /usr/local/lib/python3.6/dist-packages (1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.4) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.4) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.4) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX==1.4) (42.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: hide_code in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hide_code) (1.0.0)\n",
            "Requirement already satisfied: pdfkit in /usr/local/lib/python3.6/dist-packages (from hide_code) (0.6.1)\n",
            "Requirement already satisfied: nbconvert>=5.0 in /usr/local/lib/python3.6/dist-packages (from hide_code) (5.6.1)\n",
            "Requirement already satisfied: notebook>=5.1 in /usr/local/lib/python3.6/dist-packages (from hide_code) (5.2.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hide_code) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hide_code) (4.6.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hide_code) (4.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hide_code) (7.5.1)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.0->hide_code) (2.10.3)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.0->hide_code) (5.0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.0->hide_code) (0.4.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.0->hide_code) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.0->hide_code) (4.6.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.0->hide_code) (2.1.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.0->hide_code) (1.4.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.0->hide_code) (0.3)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.0->hide_code) (4.3.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.0->hide_code) (3.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.0->hide_code) (0.6.0)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook>=5.1->hide_code) (4.5.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook>=5.1->hide_code) (5.3.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook>=5.1->hide_code) (0.2.0)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook>=5.1->hide_code) (0.8.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hide_code) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hide_code) (1.0.18)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hide_code) (3.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert>=5.0->hide_code) (1.1.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert>=5.0->hide_code) (2.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert>=5.0->hide_code) (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert>=5.0->hide_code) (1.12.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert>=5.0->hide_code) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook>=5.1->hide_code) (2.6.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook>=5.1->hide_code) (17.0.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook>=5.1->hide_code) (0.6.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hide_code) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hide_code) (42.0.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hide_code) (4.7.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hide_code) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->hide_code) (0.1.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQFm1qb_rOE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from typing import List, Tuple\n",
        "from enigma.machine import EnigmaMachine\n",
        "from faker import Faker\n",
        "import re\n",
        "import numpy as np\n",
        "import string\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import collections\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange, tqdm\n",
        "from time import sleep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhLDj9SY8MM6",
        "colab_type": "text"
      },
      "source": [
        "## B - Helper functions already provided"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFnkSrI3rIRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConfiguredMachine:\n",
        "    def __init__(self):\n",
        "        self.machine = EnigmaMachine.from_key_sheet(\n",
        "            rotors='II IV V',\n",
        "            reflector='B',\n",
        "            ring_settings=[1, 20, 11],\n",
        "            plugboard_settings='AV BS CG DL FU HZ IN KM OW RX')\n",
        "\n",
        "    def reset(self):\n",
        "        self.machine.set_display('WXC')\n",
        "\n",
        "    def encode(self, plain_str: str) -> str:\n",
        "        self.reset()\n",
        "        return self.machine.process_text(plain_str)\n",
        "\n",
        "    def batch_encode(self, plain_list: List[str]) -> List[str]:\n",
        "        encoded = list()\n",
        "        for s in plain_list:\n",
        "            encoded.append(self.encode(s))\n",
        "        return encoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLzwW54crLAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process(input_str):\n",
        "    return re.sub('[^a-zA-Z]', '', input_str).upper()\n",
        "\n",
        "\n",
        "def generate_data(batch_size: int, seq_len: int = 42) -> Tuple[List[str], List[str]]:\n",
        "    fake = Faker()\n",
        "    machine = ConfiguredMachine()\n",
        "\n",
        "    plain_list = fake.texts(nb_texts=batch_size, max_nb_chars=seq_len)\n",
        "    plain_list = [pre_process(p) for p in plain_list]\n",
        "    cipher_list = machine.batch_encode(plain_list)\n",
        "    return plain_list, cipher_list\n",
        "\n",
        "\n",
        "def predict(cipher_list: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Solution\n",
        "    Input: Plain string with length <42\n",
        "    Output: Decryption of the string according to Enigma machine(as configured above). Size of decryption should be same as input string\n",
        "    \"\"\"\n",
        "    return cipher_list\n",
        "\n",
        "\n",
        "def str_score(str_a: str, str_b: str) -> float:\n",
        "    if len(str_a) != len(str_b):\n",
        "        return 0\n",
        "    n_correct = 0\n",
        "    for a, b in zip(str_a, str_b):\n",
        "        n_correct += int(a == b)\n",
        "    return n_correct / len(str_a)\n",
        "\n",
        "def score(predicted_plain: List[str], correct_plain: List[str]) -> float:\n",
        "    correct = 0\n",
        "    for p, c in zip(predicted_plain, correct_plain):\n",
        "        if str_score(p, c) > 0.8:\n",
        "            correct += 1\n",
        "    return correct / len(correct_plain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB4bXnRi1ZqK",
        "colab_type": "text"
      },
      "source": [
        "## C - Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8VxOEqdrxhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plain, cipher = generate_data(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXfdyJb1zVG_",
        "colab_type": "code",
        "outputId": "860d417d-5161-4194-fef1-f5464d557116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "plain[:5]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['WINGUNNORTHTREATENOUGHKID',\n",
              " 'SYSTEMSORTKEEPLOSEDRAWCHALLENGE',\n",
              " 'BEHINDTHEYTHOUGHGOOD',\n",
              " 'BAGDOFOURMARKETNEWSLOCALEAST',\n",
              " 'HARDALWAYSWHENALMOSTGIRLAGENT']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhmGE0VIzZSH",
        "colab_type": "code",
        "outputId": "31d37f1a-d92e-43a5-fb7f-6bf993d0d69f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "cipher[:5]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['IFPFBKYHZOZPMOKCUANMXPUGM',\n",
              " 'UKLOCPDHZOAOBMHALLASHMSXUDHHMAN',\n",
              " 'KAAAYGVOCMEMQTYVQKNP',\n",
              " 'KEYCTSAJZYKBWOXYUDMQTIJAIJYK',\n",
              " 'GEXCLUEPTNGMBQKFOKMHXCPAUPGVQ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf1yPloRp3Qi",
        "colab_type": "text"
      },
      "source": [
        "- The decryption of the first letter remains same for a particular varibale.\n",
        "- Therefore, there is no need to learn the key for the message becuase the Enigma machine is already set to a particular configuration\n",
        "- The data generated does not need any data cleaning and pre-preocessing since it is perfect simulated data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P-m3m5c1qMB",
        "colab_type": "text"
      },
      "source": [
        "## D - Data vectorizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6yc4Xl8twF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_characters = \"#ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "vocab = {}\n",
        "for i,char in enumerate(all_characters):\n",
        "    vocab[i]=char\n",
        "\n",
        "n_characters = len(all_characters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8acAVWMw7zW4",
        "colab_type": "text"
      },
      "source": [
        "### D.1 Turning a string into a tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSA1MEK67ndI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turning a string into a tensor\n",
        "def string_to_tensor(string):\n",
        "    \"\"\"\n",
        "    Convert string into numerical torch tensor filled with index according to above mentioned 'all_characters' variable\n",
        "    Input: String of capital letters to be dencrypted \n",
        "    Output: Torch tensor(torch.size(42)) \n",
        "    \"\"\"\n",
        "    tensor = torch.zeros(42).long()\n",
        "    for c in range(len(string)):\n",
        "        try:\n",
        "            tensor[c] = all_characters.index(string[c])\n",
        "        except:\n",
        "            continue\n",
        "    return tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGMRfCxS7p5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Readable time elapsed\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYJ3Am6f7qAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_tensor(batch_size):\n",
        "    \"\"\"\n",
        "    This the main generater function\n",
        "    Input: Batch_size (generally 2^n)\n",
        "    Output 1: Tensor of text batch padded with zero: Size=torch.size(batch_size,42)\n",
        "    Output 2: Tensor of decryption of same batch padded with zero: Size=torch.size(batch_size,42)\n",
        "    Output 3: Tensor of string sizes: Size=torch.size(batch_size)\n",
        "    Output 4: List of Strings: len(plain)=batch_size\n",
        "    \"\"\"\n",
        "    plain, cipher = generate_data(batch_size)                                   #Generates random strings and its decryption\n",
        "    x_length = []\n",
        "    for i in plain:\n",
        "        x_length.append(len(i))\n",
        "    string_length = 42\n",
        "    x = torch.LongTensor(batch_size, string_length)\n",
        "    y = torch.LongTensor(batch_size, string_length)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        y[i] = string_to_tensor(plain[i])\n",
        "        x[i] = string_to_tensor(cipher[i])\n",
        "    x = Variable(x)\n",
        "    y = Variable(y)\n",
        "    return x.long(),y.long(),torch.FloatTensor(x_length),plain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXRMjtQjOiTZ",
        "colab_type": "text"
      },
      "source": [
        "## E - Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAu1fGX99VsN",
        "colab_type": "text"
      },
      "source": [
        "- We will try to model this problem character by character prediction\n",
        "- Since the decryption of a particular character depends on the previous character and its decryption. \n",
        "- Such a situation is ideally modeled using sequence to sequence neural networks\n",
        "- These networks range from deep RNN, GRU, LSTM to deep attention based Encoder-Decoder models\n",
        "- Attention-based networks have been proved to a lot better than other encoder-decoder networks\n",
        "- These networks are capable of capturing complex sequential patterns where the next symbol is partially determined by events that happened many time steps in the past."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wrWKdtOBtx7",
        "colab_type": "text"
      },
      "source": [
        "## F - Model Architecture\n",
        "- The following are the criteria used to pick the best network: \n",
        "    - Engima decryption is highly dependent if you can learn the key formating in the machine. \n",
        "    - These keys are distributed in advance, which gives the initial configuration of the machine to decrypt the message\n",
        "    - Key list here is pre-defined in the `config` function\n",
        "- But with a constraint on ***Number of parameters<60K (Capacity of the network)***, \n",
        "    - Stacked LSTM is a good choice \n",
        "    - First, considering the learned key format in latent variables would be used throughout the decryption of the string\n",
        "    - Second, LSTM handles vanishing gradient problem much better than similar networks RNN and GRU units"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz4Jw16CZg5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size,output_size, n_layers):\n",
        "        super(CharLSTM, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        padding_idx = 0\n",
        "\n",
        "        # Encoder embedding layer is used to convert batch text in one-hot encode form\n",
        "        # Since its one-hot encoded (vocal size = 27), there no need to learn the embeddings\n",
        "        self.encoder = nn.Embedding(num_embeddings=self.input_size,\n",
        "                                    embedding_dim=self.input_size,\n",
        "                                    padding_idx=padding_idx,)\n",
        "        self.encoder.weight.data = torch.eye(self.input_size)\n",
        "        self.encoder.weight.requires_grad = False\n",
        "\n",
        "        # LSTM Unit\n",
        "        self.lstm = nn.LSTM(self.input_size, \n",
        "                           hidden_size, \n",
        "                           n_layers, \n",
        "                           batch_first=True)\n",
        "        \n",
        "        #Decoder\n",
        "        self.decoder = nn.Linear(hidden_size, \n",
        "                                 output_size)\n",
        "        \n",
        "    def forward(self, x, x_length):\n",
        "\n",
        "        # print(\"Max in x_length\",max(x_length))\n",
        "        batch_size_input = x_length.shape[0]\n",
        "        self.hidden = self.init_hidden(batch_size_input)                        # Initialize the weights\n",
        "        x = self.encoder(x)                                                     # Encoder\n",
        "\n",
        "        # print(\"/CharLSTM/forward/ Size: x after encoder - \",x.shape)\n",
        "\n",
        "        #Pack Padding will ensure that forward pass is carried for only units equal to length of string \n",
        "        x = torch.nn.utils.rnn.pack_padded_sequence(x, \n",
        "                                                    x_length, \n",
        "                                                    batch_first=True,\n",
        "                                                    enforce_sorted=False)\n",
        "        \n",
        "        x, self.hidden = self.lstm(x, self.hidden)                              # Running the LSTM Unit\n",
        "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "        # print(\"/CharLSTM/forward/ Size: x after pad - \",x.shape)\n",
        "\n",
        "        x = x.contiguous()\n",
        "        x = x.view(-1, x.shape[2])\n",
        "        # print(\"/CharLSTM/forward/ Size: x after view - \",x.shape)\n",
        "        x = self.decoder(x)                                                     # Decoder\n",
        "        # print(\"/CharLSTM/forward/ Size: x after decoder - \",x.shape)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        # x = F.softmax(x, dim=1)\n",
        "\n",
        "        Y_hat = x\n",
        "        return Y_hat\n",
        "\n",
        "\n",
        "    def init_hidden(self,batch_size_input):\n",
        "        \"\"\"\n",
        "        Initial the weights\n",
        "        Method: Random\n",
        "        \"\"\"\n",
        "        hidden_a = torch.randn(self.n_layers,batch_size_input, self.hidden_size)\n",
        "        hidden_b = torch.randn(self.n_layers,batch_size_input, self.hidden_size)\n",
        "\n",
        "        hidden_a = Variable(hidden_a)\n",
        "        hidden_b = Variable(hidden_b)\n",
        "\n",
        "        return (hidden_a, hidden_b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xOz4F1qJGcY",
        "colab_type": "text"
      },
      "source": [
        "## G - Cross Entropy Loss\n",
        "- Since the size of each string is different, the output prediction should be padded with zeros\n",
        "- Inbuilt cross-entropy function is not suitable here because of the difference in shapes of `Y_hat` and `Y`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37HZN2wBFOuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def criterion(Y_hat, Y, X_lengths,batch_size):\n",
        "    \"\"\"\n",
        "    Custom defined Cross entropy Function (Objective function calculation)\n",
        "    Input 1: Tensor of prediction of a batch of data: Size=torch.size(batch_size,m,n_characters)\n",
        "    Input 2: Tensor of groud truth of a batch of data: Size=torch.size(batch_size,42,n_characters)\n",
        "    Input 3: Tensor of string sizes: Size=torch.size(batch_size)\n",
        "    Ouput: Single digit tensor - cross entropy loss\n",
        "    \"\"\"\n",
        "    m = int(max(X_lengths).detach())\n",
        "    Y_hat = Y_hat.view(batch_size,m,input_size)\n",
        "    # print(\"/criterion/ Size: Y_hat after view - \",Y_hat.shape)\n",
        "    # print(\"/criterion/ Check the sum of softmax==1 - \",torch.sum(Y_hat[0,0,:]))\n",
        "\n",
        "    # npad is a tuple of 2*len(n_before, n_after) for each dimension\n",
        "    Y_hat = F.pad(input=Y_hat, pad=(0, 0, 0, 42-m,0,0), mode='constant', value=0)\n",
        "    # print(\"/criterion/ Size: Y_hat after padding - \",Y_hat.shape)\n",
        "\n",
        "    one_hot_encoder = nn.Embedding(num_embeddings=input_size,\n",
        "                                embedding_dim=input_size)\n",
        "    one_hot_encoder.weight.data = torch.eye(input_size)\n",
        "    one_hot_encoder.weight.requires_grad = False\n",
        "    Y = one_hot_encoder(Y)\n",
        "    # print(\"/criterion/ Size: Y after nn.embeddings - \",Y.shape)\n",
        "\n",
        "\n",
        "    Y = Y.view(-1)\n",
        "    Y_hat = Y_hat.view(-1)\n",
        "    total_char = int(torch.sum(Y))\n",
        "    ce_loss = Y*Y_hat\n",
        "    # print(\"/criterion/ Dimension of Ce_loss\",ce_loss.shape)\n",
        "    ce_loss = -torch.sum(ce_loss) / total_char\n",
        "    return ce_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC1eK6I_KCTa",
        "colab_type": "text"
      },
      "source": [
        "## H - Model function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjFmX3VRh8Sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(params):\n",
        "    \"\"\"\n",
        "    Builts model with given hyperparamters\n",
        "    Input: Dictionary of Hyperparamters. Here I am considering 3 hyperparameters: Number of layers, hidden size and batch size\n",
        "    Output1: LSTM model\n",
        "    Output2: Adam optimizer\n",
        "    Output3: Learning rate scheduler\n",
        "    \"\"\"\n",
        "    # Hyperparameters\n",
        "    input_size = 27\n",
        "    output_size = 27\n",
        "    n_characters=27\n",
        "    hidden_size = params['hiddensize']\n",
        "    n_layers = params['nlayers']\n",
        "    batch_size = params['batchsize']\n",
        "    learning_rate=0.01\n",
        "\n",
        "    # model\n",
        "    final_model = CharLSTM(input_size,\n",
        "                          hidden_size, \n",
        "                          output_size,\n",
        "                          n_layers)\n",
        "\n",
        "    # Adam Optimizer with learning rate scheduler\n",
        "    optimizer = torch.optim.Adam(final_model.parameters(), lr=learning_rate,betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.99, last_epoch=-1)\n",
        "    return final_model,optimizer,scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ajO11WkOtoA",
        "colab_type": "text"
      },
      "source": [
        "## I - Printing Model and its paramters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jod_WhhCPiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def printing_model(model):\n",
        "    \"\"\"\n",
        "    Prints different characteristics of the LSTM Model\n",
        "    \"\"\"\n",
        "    print(\"Printing Model\")\n",
        "    print(model)\n",
        "    print('-'*160)\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(\"Number of Paramters: \",pytorch_total_params)\n",
        "    print('-'*160)\n",
        "    for i in range(len(list(model.parameters()))):\n",
        "        print(list(model.parameters())[i].size())\n",
        "    print('-'*160)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miu8bbhdO0XV",
        "colab_type": "text"
      },
      "source": [
        "## J - Save computational graph for Tensorboard Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_LXH6te-g-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "def save_graph(root,model,batch_size):\n",
        "    \"\"\"\n",
        "    Saving computational graph for visualization in TensorBoard\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    x1,y1,x_length1,_ = batch_tensor(batch_size)\n",
        "    writer = SummaryWriter(root)\n",
        "    writer.add_graph(model,input_to_model=(x1,x_length1),verbose=True)\n",
        "    writer.close()\n",
        "    print('-'*160)\n",
        "    return writer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wygtGNKLeQ",
        "colab_type": "text"
      },
      "source": [
        "## K - Functions of Score and Accuracy calcultions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pPX1T9JPmsB",
        "colab_type": "text"
      },
      "source": [
        "### K.1 - Accuracy at character level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMJsH4qcUUCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def string_accuracy(predicted_plain: List[str], correct_plain: List[str],x_length) -> float:\n",
        "    x_length_array = x_length.detach().numpy().astype('int') \n",
        "    correct = 0\n",
        "    for p, c in zip(predicted_plain, correct_plain):\n",
        "        for k in range(len(p)):\n",
        "            if p[k]==c[k]:\n",
        "                correct = correct +1\n",
        "    return correct / sum(x_length_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVAZ99O6PtpH",
        "colab_type": "text"
      },
      "source": [
        "### K.2 - Converts one-hot prediction output to Strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSW-MUByUUK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def numerical_string(batch_size,y_pred,y_batch_string,x_length):\n",
        "    x_length_array = x_length.detach().numpy().astype('int') \n",
        "    m = max(x_length_array)\n",
        "    y_pred = y_pred.view(batch_size,m,input_size)\n",
        "    n_y_pred = []\n",
        "\n",
        "    for k in range(batch_size):\n",
        "        string_pred = \"\" \n",
        "        for i in range(x_length_array[k]):\n",
        "            string_pred = string_pred+vocab[np.argmax(y_pred[k][i].detach().numpy())]\n",
        "\n",
        "        n_y_pred.append(string_pred)\n",
        "\n",
        "    return n_y_pred,y_batch_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy10E-cXQAe9",
        "colab_type": "text"
      },
      "source": [
        "## L - Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faiNbYqDFjRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(final_model, Iteration, batch_size,loss_fn, optimizer,writer_comment):\n",
        "    iter = 0\n",
        "    overall_accuracy = 0\n",
        "    char_accuracy = 0\n",
        "    running_metric = collections.deque(16*[2], 16)\n",
        "    running_character_accuracy = collections.deque(8*[0], 8)\n",
        "    for iter in range(Iteration):\n",
        "\n",
        "        x,y,x_length,plain = batch_tensor(batch_size)\n",
        "\n",
        "        Y_hat = final_model(x,x_length)\n",
        "\n",
        "        # print(\"/train/ Shape: Y_hat - \",Y_hat.shape)\n",
        "        # print(/train/ Shape: y\",y.shape)\n",
        "        final_model.zero_grad()\n",
        "        loss = loss_fn(Y_hat, y, x_length,batch_size)\n",
        "        batch_accuracy = score(*numerical_string(batch_size,Y_hat,plain,x_length))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch_accuracy = score(*numerical_string(batch_size,Y_hat,plain,x_length))\n",
        "        char_accuracy = (char_accuracy*iter+string_accuracy(*numerical_string(batch_size,Y_hat,plain,x_length),x_length))/(iter+1)\n",
        "\n",
        "        overall_accuracy = (overall_accuracy*iter+batch_accuracy)/(iter+1)\n",
        "\n",
        "        if iter % 20 == 0:\n",
        "  \n",
        "            test_x,test_y,test_x_length,test_plain = batch_tensor(batch_size)\n",
        "            y_hat_test = final_model(test_x,test_x_length)\n",
        "            loss_test = loss_fn(y_hat_test, test_y, test_x_length,batch_size)\n",
        "\n",
        "            batch_accuracy = score(*numerical_string(batch_size,Y_hat,plain,x_length))\n",
        "            batch_accuracy_test = score(*numerical_string(batch_size,y_hat_test,test_plain,test_x_length))\n",
        "\n",
        "\n",
        "            char_accuracy_test = string_accuracy(*numerical_string(batch_size,y_hat_test,test_plain,test_x_length),test_x_length)\n",
        "\n",
        "            writer.add_scalars(writer_comment+' Running Cross Entropy loss',\n",
        "                              {\n",
        "                                  \"Training\":loss,\n",
        "                                  \"Test\":loss_test\n",
        "                              },\n",
        "                              iter)\n",
        "            running_metric.append(loss)\n",
        "            running_metric.append(loss_test)\n",
        "            writer.add_scalars(writer_comment+' Running Batch Accuracy',\n",
        "                              {\n",
        "                                  \"Training\":batch_accuracy,\n",
        "                                  \"Test\":batch_accuracy_test\n",
        "                              },\n",
        "                              iter)\n",
        "            \n",
        "            writer.add_scalars(writer_comment+' Character Accuracy',\n",
        "                              {\n",
        "                                  \"Training\":char_accuracy,\n",
        "                                  \"Test\":char_accuracy_test\n",
        "                              },\n",
        "                              iter)\n",
        "            running_character_accuracy.append(char_accuracy_test)\n",
        "            writer.add_scalar(writer_comment+' Overall Accuracy',\n",
        "                              overall_accuracy,\n",
        "                              iter)\n",
        "\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration:{:>4} || Cross Entropy loss- Train: {:04.3f} Test: {:04.3f} || Batch Score- Train: {:5.1f}% Test: {:5.1f}% || Cumulative Score {:5.1f}% || Character_accuracy - Train(cumulative): {:5.1f}%   Test: {:5.1f}% '.format(iter, loss,loss_test,batch_accuracy*100.0,batch_accuracy_test*100.0,overall_accuracy*100.0,char_accuracy*100.0,char_accuracy_test*100.0))\n",
        "        \n",
        "            if sum(running_metric) / len(running_metric)<0.09:\n",
        "                print('-'*160)\n",
        "                print(\"Stopping Condition Achieved: Average of Last 16 (2*8) Cross entropy Loss is less than 0.09: \",sum(running_metric) / len(running_metric))\n",
        "                print('-'*160)\n",
        "                return sum(running_character_accuracy) / len(running_character_accuracy)\n",
        "\n",
        "            if sum(running_metric) / len(running_metric)>1.3 and iter>1000:\n",
        "                print('-'*160)\n",
        "                print(\"The model with current paramters is not converging in first 1000 iterations: \",sum(running_metric) / len(running_metric))\n",
        "                print('-'*160)\n",
        "                return sum(running_character_accuracy) / len(running_character_accuracy)\n",
        "\n",
        "            if sum(running_metric) / len(running_metric)>0.5 and iter>2000:\n",
        "                print('-'*160)\n",
        "                print(\"The model with current paramters is not converging in first 2000 iterations: \",sum(running_metric) / len(running_metric))\n",
        "                print('-'*160)\n",
        "                return sum(running_character_accuracy) / len(running_character_accuracy)\n",
        "\n",
        "            if iter>4400 or iter==(Iteration-1):\n",
        "                print('-'*160)\n",
        "                print(\"The model with current paramters is not converging: \",sum(running_metric) / len(running_metric))\n",
        "                print('-'*160)\n",
        "                return sum(running_character_accuracy) / len(running_character_accuracy)\n",
        "        if iter%50==0:\n",
        "            scheduler.step()\n",
        "\n",
        "        time.sleep(0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_GvNcsfQHfP",
        "colab_type": "text"
      },
      "source": [
        "## M - Actual model training for a set of paramters and Backpropogation through time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHgdZMPOU4PL",
        "colab_type": "text"
      },
      "source": [
        "### M.1 Model variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VXeHP7YUr8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_dict={  \n",
        "        'hiddensize':64,\n",
        "        'nlayers':2,\n",
        "        'batchsize':32,\n",
        "}\n",
        "input_size = 27\n",
        "output_size = 27\n",
        "n_characters=27\n",
        "hidden_size = param_dict['hiddensize']\n",
        "n_layers = param_dict['nlayers']\n",
        "batch_size = param_dict['batchsize']\n",
        "learning_rate=0.01\n",
        "comment ='layers'+str(n_layers)+'_hiddensize'+str(hidden_size)+'_batchsize'+str(batch_size)\n",
        "path = root+comment+'_saved_model.pth'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xQIG35hU9Rq",
        "colab_type": "text"
      },
      "source": [
        "### M.2 Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP6nVrK_uGsU",
        "colab_type": "code",
        "outputId": "9bcf7aba-74e1-451e-ad6d-94188ac4667c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "final_model,optimizer,scheduler = model(param_dict)\n",
        "\n",
        "printing_model(final_model)\n",
        "\n",
        "writer = save_graph(root,final_model,batch_size)\n",
        "\n",
        "\n",
        "character_acc = train(final_model, 5000, batch_size,criterion, optimizer,comment)\n",
        "print(character_acc)\n",
        "torch.save(final_model, path)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing Model\n",
            "CharLSTM(\n",
            "  (encoder): Embedding(27, 27, padding_idx=0)\n",
            "  (lstm): LSTM(27, 64, num_layers=2, batch_first=True)\n",
            "  (decoder): Linear(in_features=64, out_features=27, bias=True)\n",
            ")\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Number of Paramters:  59572\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "torch.Size([27, 27])\n",
            "torch.Size([256, 27])\n",
            "torch.Size([256, 64])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 64])\n",
            "torch.Size([256, 64])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([27, 64])\n",
            "torch.Size([27])\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "graph(%self : ClassType<CharLSTM>,\n",
            "      %input.1 : Long(32, 42),\n",
            "      %x_length : Float(32)):\n",
            "  %1 : ClassType<Embedding> = prim::GetAttr[name=\"encoder\"](%self)\n",
            "  %weight.1 : Tensor = prim::GetAttr[name=\"weight\"](%1)\n",
            "  %3 : ClassType<LSTM> = prim::GetAttr[name=\"lstm\"](%self)\n",
            "  %4 : Tensor = prim::GetAttr[name=\"bias_hh_l0\"](%3)\n",
            "  %5 : Tensor = prim::GetAttr[name=\"bias_hh_l1\"](%3)\n",
            "  %6 : Tensor = prim::GetAttr[name=\"bias_ih_l0\"](%3)\n",
            "  %7 : Tensor = prim::GetAttr[name=\"bias_ih_l1\"](%3)\n",
            "  %8 : Tensor = prim::GetAttr[name=\"weight_hh_l0\"](%3)\n",
            "  %9 : Tensor = prim::GetAttr[name=\"weight_hh_l1\"](%3)\n",
            "  %10 : Tensor = prim::GetAttr[name=\"weight_ih_l0\"](%3)\n",
            "  %11 : Tensor = prim::GetAttr[name=\"weight_ih_l1\"](%3)\n",
            "  %12 : ClassType<Linear> = prim::GetAttr[name=\"decoder\"](%self)\n",
            "  %weight : Tensor = prim::GetAttr[name=\"weight\"](%12)\n",
            "  %bias : Tensor = prim::GetAttr[name=\"bias\"](%12)\n",
            "  %17 : int = prim::Constant[value=0](), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:33:0\n",
            "  %18 : int = aten::size(%x_length, %17), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:33:0\n",
            "  %batch_size_input : Long() = prim::NumToTensor(%18), scope: CharLSTM\n",
            "  %29 : int = aten::Int(%batch_size_input), scope: CharLSTM\n",
            "  %20 : int = aten::Int(%batch_size_input), scope: CharLSTM\n",
            "  %21 : int = prim::Constant[value=2](), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:67:0\n",
            "  %22 : int = prim::Constant[value=64](), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:67:0\n",
            "  %23 : int[] = prim::ListConstruct(%21, %20, %22), scope: CharLSTM\n",
            "  %24 : int = prim::Constant[value=6](), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:67:0\n",
            "  %25 : int = prim::Constant[value=0](), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:67:0\n",
            "  %26 : Device = prim::Constant[value=\"cpu\"](), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:67:0\n",
            "  %27 : bool = prim::Constant[value=0](), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:67:0\n",
            "  %hidden_a : Float(2, 32, 64) = aten::randn(%23, %24, %25, %26, %27), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:67:0\n",
            "  %30 : int = prim::Constant[value=2](), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:68:0\n",
            "  %31 : int = prim::Constant[value=64](), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:68:0\n",
            "  %32 : int[] = prim::ListConstruct(%30, %29, %31), scope: CharLSTM\n",
            "  %33 : int = prim::Constant[value=6](), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:68:0\n",
            "  %34 : int = prim::Constant[value=0](), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:68:0\n",
            "  %35 : Device = prim::Constant[value=\"cpu\"](), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:68:0\n",
            "  %36 : bool = prim::Constant[value=0](), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:68:0\n",
            "  %hidden_b : Float(2, 32, 64) = aten::randn(%32, %33, %34, %35, %36), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:68:0\n",
            "  %38 : int = prim::Constant[value=0](), scope: CharLSTM/Embedding[encoder] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1484:0\n",
            "  %39 : bool = prim::Constant[value=0](), scope: CharLSTM/Embedding[encoder] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1484:0\n",
            "  %40 : bool = prim::Constant[value=0](), scope: CharLSTM/Embedding[encoder] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1484:0\n",
            "  %input.2 : Float(32, 42, 27) = aten::embedding(%weight.1, %input.1, %38, %39, %40), scope: CharLSTM/Embedding[encoder] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1484:0\n",
            "  %42 : Device = prim::Constant[value=\"cpu\"](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:272:0\n",
            "  %43 : int = prim::Constant[value=4](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:272:0\n",
            "  %44 : bool = prim::Constant[value=0](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:272:0\n",
            "  %45 : bool = prim::Constant[value=0](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:272:0\n",
            "  %lengths.1 : Long(32) = aten::to(%x_length, %42, %43, %44, %45), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:272:0\n",
            "  %47 : int = prim::Constant[value=-1](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:276:0\n",
            "  %48 : bool = prim::Constant[value=1](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:276:0\n",
            "  %lengths.2 : Long(32), %sorted_indices.1 : Long(32) = aten::sort(%lengths.1, %47, %48), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:276:0\n",
            "  %51 : int = prim::Constant[value=4](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:277:0\n",
            "  %52 : int = prim::Constant[value=0](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:277:0\n",
            "  %53 : Device = prim::Constant[value=\"cpu\"](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:277:0\n",
            "  %54 : bool = prim::Constant[value=0](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:277:0\n",
            "  %55 : bool = prim::Constant[value=0](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:277:0\n",
            "  %56 : bool = prim::Constant[value=0](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:277:0\n",
            "  %sorted_indices : Long(32) = aten::to(%sorted_indices.1, %51, %52, %53, %54, %55, %56), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:277:0\n",
            "  %58 : int = prim::Constant[value=0](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:279:0\n",
            "  %input.3 : Float(32, 42, 27) = aten::index_select(%input.2, %58, %sorted_indices), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:279:0\n",
            "  %60 : bool = prim::Constant[value=1](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:282:0\n",
            "  %input.4 : Float(755, 27), %batch_sizes : Long(34) = aten::_pack_padded_sequence(%input.3, %lengths.2, %60), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:282:0\n",
            "  %63 : int = prim::Constant[value=4](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:228:0\n",
            "  %64 : int = prim::Constant[value=0](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:228:0\n",
            "  %65 : Device = prim::Constant[value=\"cpu\"](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:228:0\n",
            "  %66 : bool = prim::Constant[value=0](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:228:0\n",
            "  %67 : None = prim::Constant(), scope: CharLSTM\n",
            "  %output : Long(32) = aten::empty_like(%sorted_indices, %63, %64, %65, %66, %67), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:228:0\n",
            "  %69 : int = prim::Constant[value=0](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:230:0\n",
            "  %70 : int = prim::Constant[value=32](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:230:0\n",
            "  %71 : int = prim::Constant[value=1](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:230:0\n",
            "  %72 : int = prim::Constant[value=4](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:230:0\n",
            "  %73 : int = prim::Constant[value=0](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:230:0\n",
            "  %74 : Device = prim::Constant[value=\"cpu\"](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:230:0\n",
            "  %75 : bool = prim::Constant[value=0](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:230:0\n",
            "  %76 : Long(32) = aten::arange(%69, %70, %71, %72, %73, %74, %75), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:230:0\n",
            "  %77 : int = prim::Constant[value=0](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:230:0\n",
            "  %permutation : Long(32) = aten::scatter_(%output, %77, %sorted_indices, %76), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:230:0\n",
            "  %82 : int = prim::Constant[value=1](), scope: CharLSTM/LSTM[lstm] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:21:0\n",
            "  %hx.1 : Float(2, 32, 64) = aten::index_select(%hidden_a, %82, %sorted_indices), scope: CharLSTM/LSTM[lstm] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:21:0\n",
            "  %84 : int = prim::Constant[value=1](), scope: CharLSTM/LSTM[lstm] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:21:0\n",
            "  %hx : Float(2, 32, 64) = aten::index_select(%hidden_b, %84, %sorted_indices), scope: CharLSTM/LSTM[lstm] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:21:0\n",
            "  %124 : Tensor[] = prim::ListConstruct(%hx.1, %hx), scope: CharLSTM/LSTM[lstm]\n",
            "  %125 : Tensor[] = prim::ListConstruct(%10, %8, %6, %4, %11, %9, %7, %5), scope: CharLSTM/LSTM[lstm]\n",
            "  %126 : bool = prim::Constant[value=1](), scope: CharLSTM/LSTM[lstm] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:529:0\n",
            "  %127 : int = prim::Constant[value=2](), scope: CharLSTM/LSTM[lstm] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:529:0\n",
            "  %128 : float = prim::Constant[value=0](), scope: CharLSTM/LSTM[lstm] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:529:0\n",
            "  %129 : bool = prim::Constant[value=0](), scope: CharLSTM/LSTM[lstm] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:529:0\n",
            "  %130 : bool = prim::Constant[value=0](), scope: CharLSTM/LSTM[lstm] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:529:0\n",
            "  %131 : Float(755, 64), %tensor.1 : Float(2, 32, 64), %tensor : Float(2, 32, 64) = aten::lstm(%input.4, %batch_sizes, %124, %125, %126, %127, %128, %129, %130), scope: CharLSTM/LSTM[lstm] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:529:0\n",
            "  %138 : int = prim::Constant[value=0](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:320:0\n",
            "  %139 : int = aten::size(%batch_sizes, %138), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:320:0\n",
            "  %max_seq_length : Long() = prim::NumToTensor(%139), scope: CharLSTM\n",
            "  %141 : int = aten::Int(%max_seq_length), scope: CharLSTM\n",
            "  %142 : bool = prim::Constant[value=1](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:329:0\n",
            "  %143 : float = prim::Constant[value=0](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:329:0\n",
            "  %padded_output : Float(32, 34, 64), %lengths : Long(32) = aten::_pad_packed_sequence(%131, %batch_sizes, %142, %143, %141), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:329:0\n",
            "  %146 : int = prim::Constant[value=0](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:333:0\n",
            "  %x.1 : Float(32, 34, 64) = aten::index_select(%padded_output, %146, %permutation), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py:333:0\n",
            "  %157 : int = prim::Constant[value=0](), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:50:0\n",
            "  %x : Float(32, 34, 64) = aten::contiguous(%x.1, %157), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:50:0\n",
            "  %165 : int = prim::Constant[value=2](), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:51:0\n",
            "  %166 : int = aten::size(%x, %165), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:51:0\n",
            "  %167 : Long() = prim::NumToTensor(%166), scope: CharLSTM\n",
            "  %168 : int = aten::Int(%167), scope: CharLSTM\n",
            "  %169 : int = prim::Constant[value=-1](), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:51:0\n",
            "  %170 : int[] = prim::ListConstruct(%169, %168), scope: CharLSTM\n",
            "  %input.5 : Float(1088, 64) = aten::view(%x, %170), scope: CharLSTM # <ipython-input-60-cdf22d77fd80>:51:0\n",
            "  %172 : Float(64, 27) = aten::t(%weight), scope: CharLSTM/Linear[decoder] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1370:0\n",
            "  %173 : int = prim::Constant[value=1](), scope: CharLSTM/Linear[decoder] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1370:0\n",
            "  %174 : int = prim::Constant[value=1](), scope: CharLSTM/Linear[decoder] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1370:0\n",
            "  %input : Float(1088, 27) = aten::addmm(%bias, %input.5, %172, %173, %174), scope: CharLSTM/Linear[decoder] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1370:0\n",
            "  %176 : int = prim::Constant[value=1](), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1317:0\n",
            "  %177 : None = prim::Constant(), scope: CharLSTM\n",
            "  %178 : Float(1088, 27) = aten::log_softmax(%input, %176, %177), scope: CharLSTM # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1317:0\n",
            "  return (%178)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Iteration:   0 || Cross Entropy loss- Train: 2.794 Test: 2.761 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):   3.0%   Test:  12.6% \n",
            "Iteration:  20 || Cross Entropy loss- Train: 2.421 Test: 2.418 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  13.1%   Test:  12.5% \n",
            "Iteration:  40 || Cross Entropy loss- Train: 2.421 Test: 2.280 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  13.3%   Test:  12.7% \n",
            "Iteration:  60 || Cross Entropy loss- Train: 2.204 Test: 2.114 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  14.1%   Test:  15.6% \n",
            "Iteration:  80 || Cross Entropy loss- Train: 2.069 Test: 2.001 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  15.8%   Test:  23.6% \n",
            "Iteration: 100 || Cross Entropy loss- Train: 1.881 Test: 1.910 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  17.3%   Test:  26.1% \n",
            "Iteration: 120 || Cross Entropy loss- Train: 1.771 Test: 1.832 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  18.6%   Test:  25.6% \n",
            "Iteration: 140 || Cross Entropy loss- Train: 1.661 Test: 1.612 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  20.3%   Test:  34.1% \n",
            "Iteration: 160 || Cross Entropy loss- Train: 1.426 Test: 1.546 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  22.6%   Test:  41.2% \n",
            "Iteration: 180 || Cross Entropy loss- Train: 1.200 Test: 1.284 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  25.4%   Test:  49.1% \n",
            "Iteration: 200 || Cross Entropy loss- Train: 1.119 Test: 1.054 || Batch Score- Train:   9.4% Test:   6.2% || Cumulative Score   0.2% || Character_accuracy - Train(cumulative):  28.4%   Test:  57.5% \n",
            "Iteration: 220 || Cross Entropy loss- Train: 0.940 Test: 0.880 || Batch Score- Train:   6.2% Test:  15.6% || Cumulative Score   0.9% || Character_accuracy - Train(cumulative):  31.4%   Test:  66.5% \n",
            "Iteration: 240 || Cross Entropy loss- Train: 0.812 Test: 0.752 || Batch Score- Train:  18.8% Test:   9.4% || Cumulative Score   2.2% || Character_accuracy - Train(cumulative):  34.4%   Test:  70.2% \n",
            "Iteration: 260 || Cross Entropy loss- Train: 0.662 Test: 0.717 || Batch Score- Train:  34.4% Test:  15.6% || Cumulative Score   4.0% || Character_accuracy - Train(cumulative):  37.3%   Test:  72.7% \n",
            "Iteration: 280 || Cross Entropy loss- Train: 0.568 Test: 0.584 || Batch Score- Train:  46.9% Test:  53.1% || Cumulative Score   6.6% || Character_accuracy - Train(cumulative):  40.1%   Test:  79.7% \n",
            "Iteration: 300 || Cross Entropy loss- Train: 0.447 Test: 0.570 || Batch Score- Train:  71.9% Test:  46.9% || Cumulative Score   9.8% || Character_accuracy - Train(cumulative):  42.6%   Test:  78.8% \n",
            "Iteration: 320 || Cross Entropy loss- Train: 0.522 Test: 0.557 || Batch Score- Train:  62.5% Test:  75.0% || Cumulative Score  13.3% || Character_accuracy - Train(cumulative):  45.1%   Test:  80.9% \n",
            "Iteration: 340 || Cross Entropy loss- Train: 0.471 Test: 0.452 || Batch Score- Train:  68.8% Test:  68.8% || Cumulative Score  16.6% || Character_accuracy - Train(cumulative):  47.4%   Test:  83.3% \n",
            "Iteration: 360 || Cross Entropy loss- Train: 0.346 Test: 0.491 || Batch Score- Train:  84.4% Test:  62.5% || Cumulative Score  19.8% || Character_accuracy - Train(cumulative):  49.4%   Test:  81.6% \n",
            "Iteration: 380 || Cross Entropy loss- Train: 0.410 Test: 0.372 || Batch Score- Train:  78.1% Test:  81.2% || Cumulative Score  23.0% || Character_accuracy - Train(cumulative):  51.4%   Test:  86.0% \n",
            "Iteration: 400 || Cross Entropy loss- Train: 0.375 Test: 0.348 || Batch Score- Train:  81.2% Test:  81.2% || Cumulative Score  26.2% || Character_accuracy - Train(cumulative):  53.2%   Test:  86.4% \n",
            "Iteration: 420 || Cross Entropy loss- Train: 0.312 Test: 0.335 || Batch Score- Train:  93.8% Test:  90.6% || Cumulative Score  29.3% || Character_accuracy - Train(cumulative):  54.9%   Test:  88.1% \n",
            "Iteration: 440 || Cross Entropy loss- Train: 0.270 Test: 0.232 || Batch Score- Train:  90.6% Test:  96.9% || Cumulative Score  32.2% || Character_accuracy - Train(cumulative):  56.5%   Test:  92.6% \n",
            "Iteration: 460 || Cross Entropy loss- Train: 0.255 Test: 0.230 || Batch Score- Train:  87.5% Test:  96.9% || Cumulative Score  34.7% || Character_accuracy - Train(cumulative):  57.9%   Test:  92.3% \n",
            "Iteration: 480 || Cross Entropy loss- Train: 0.287 Test: 0.235 || Batch Score- Train:  93.8% Test:  93.8% || Cumulative Score  37.1% || Character_accuracy - Train(cumulative):  59.3%   Test:  92.1% \n",
            "Iteration: 500 || Cross Entropy loss- Train: 0.246 Test: 0.218 || Batch Score- Train:  93.8% Test:  93.8% || Cumulative Score  39.4% || Character_accuracy - Train(cumulative):  60.6%   Test:  91.7% \n",
            "Iteration: 520 || Cross Entropy loss- Train: 0.324 Test: 0.140 || Batch Score- Train:  90.6% Test: 100.0% || Cumulative Score  41.6% || Character_accuracy - Train(cumulative):  61.8%   Test:  95.5% \n",
            "Iteration: 540 || Cross Entropy loss- Train: 0.284 Test: 0.247 || Batch Score- Train:  90.6% Test:  90.6% || Cumulative Score  43.5% || Character_accuracy - Train(cumulative):  62.8%   Test:  90.4% \n",
            "Iteration: 560 || Cross Entropy loss- Train: 0.195 Test: 0.256 || Batch Score- Train:  96.9% Test:  96.9% || Cumulative Score  45.3% || Character_accuracy - Train(cumulative):  63.9%   Test:  90.4% \n",
            "Iteration: 580 || Cross Entropy loss- Train: 0.276 Test: 0.206 || Batch Score- Train:  93.8% Test:  96.9% || Cumulative Score  47.1% || Character_accuracy - Train(cumulative):  64.8%   Test:  92.5% \n",
            "Iteration: 600 || Cross Entropy loss- Train: 0.234 Test: 0.353 || Batch Score- Train:  90.6% Test:  90.6% || Cumulative Score  48.7% || Character_accuracy - Train(cumulative):  65.8%   Test:  87.1% \n",
            "Iteration: 620 || Cross Entropy loss- Train: 0.240 Test: 0.181 || Batch Score- Train:  90.6% Test: 100.0% || Cumulative Score  50.2% || Character_accuracy - Train(cumulative):  66.6%   Test:  94.0% \n",
            "Iteration: 640 || Cross Entropy loss- Train: 0.211 Test: 0.193 || Batch Score- Train:  93.8% Test:  96.9% || Cumulative Score  51.4% || Character_accuracy - Train(cumulative):  67.3%   Test:  93.5% \n",
            "Iteration: 660 || Cross Entropy loss- Train: 0.187 Test: 0.261 || Batch Score- Train:  96.9% Test:  93.8% || Cumulative Score  52.7% || Character_accuracy - Train(cumulative):  68.1%   Test:  90.5% \n",
            "Iteration: 680 || Cross Entropy loss- Train: 0.704 Test: 0.180 || Batch Score- Train:  56.2% Test:  93.8% || Cumulative Score  53.9% || Character_accuracy - Train(cumulative):  68.7%   Test:  93.3% \n",
            "Iteration: 700 || Cross Entropy loss- Train: 0.263 Test: 0.193 || Batch Score- Train:  93.8% Test:  96.9% || Cumulative Score  54.9% || Character_accuracy - Train(cumulative):  69.3%   Test:  91.5% \n",
            "Iteration: 720 || Cross Entropy loss- Train: 0.193 Test: 0.128 || Batch Score- Train:  93.8% Test: 100.0% || Cumulative Score  56.0% || Character_accuracy - Train(cumulative):  69.9%   Test:  95.4% \n",
            "Iteration: 740 || Cross Entropy loss- Train: 0.165 Test: 0.214 || Batch Score- Train:  96.9% Test:  96.9% || Cumulative Score  57.1% || Character_accuracy - Train(cumulative):  70.6%   Test:  91.6% \n",
            "Iteration: 760 || Cross Entropy loss- Train: 0.224 Test: 0.192 || Batch Score- Train: 100.0% Test:  96.9% || Cumulative Score  57.9% || Character_accuracy - Train(cumulative):  71.0%   Test:  92.0% \n",
            "Iteration: 780 || Cross Entropy loss- Train: 0.185 Test: 0.148 || Batch Score- Train:  93.8% Test: 100.0% || Cumulative Score  58.9% || Character_accuracy - Train(cumulative):  71.6%   Test:  94.6% \n",
            "Iteration: 800 || Cross Entropy loss- Train: 0.147 Test: 0.166 || Batch Score- Train: 100.0% Test: 100.0% || Cumulative Score  59.8% || Character_accuracy - Train(cumulative):  72.1%   Test:  94.6% \n",
            "Iteration: 820 || Cross Entropy loss- Train: 0.138 Test: 0.217 || Batch Score- Train: 100.0% Test:  96.9% || Cumulative Score  60.8% || Character_accuracy - Train(cumulative):  72.7%   Test:  92.6% \n",
            "Iteration: 840 || Cross Entropy loss- Train: 0.122 Test: 0.197 || Batch Score- Train: 100.0% Test:  96.9% || Cumulative Score  61.7% || Character_accuracy - Train(cumulative):  73.2%   Test:  92.6% \n",
            "Iteration: 860 || Cross Entropy loss- Train: 0.102 Test: 0.119 || Batch Score- Train: 100.0% Test: 100.0% || Cumulative Score  62.5% || Character_accuracy - Train(cumulative):  73.7%   Test:  96.0% \n",
            "Iteration: 880 || Cross Entropy loss- Train: 0.114 Test: 0.129 || Batch Score- Train: 100.0% Test: 100.0% || Cumulative Score  63.3% || Character_accuracy - Train(cumulative):  74.2%   Test:  95.4% \n",
            "Iteration: 900 || Cross Entropy loss- Train: 0.115 Test: 0.093 || Batch Score- Train: 100.0% Test: 100.0% || Cumulative Score  64.1% || Character_accuracy - Train(cumulative):  74.7%   Test:  96.5% \n",
            "Iteration: 920 || Cross Entropy loss- Train: 0.095 Test: 0.097 || Batch Score- Train: 100.0% Test: 100.0% || Cumulative Score  64.9% || Character_accuracy - Train(cumulative):  75.2%   Test:  96.4% \n",
            "Iteration: 940 || Cross Entropy loss- Train: 0.150 Test: 0.104 || Batch Score- Train:  96.9% Test:  96.9% || Cumulative Score  65.6% || Character_accuracy - Train(cumulative):  75.6%   Test:  96.0% \n",
            "Iteration: 960 || Cross Entropy loss- Train: 0.132 Test: 0.231 || Batch Score- Train: 100.0% Test:  93.8% || Cumulative Score  66.2% || Character_accuracy - Train(cumulative):  76.0%   Test:  92.9% \n",
            "Iteration: 980 || Cross Entropy loss- Train: 0.141 Test: 0.101 || Batch Score- Train:  96.9% Test: 100.0% || Cumulative Score  66.9% || Character_accuracy - Train(cumulative):  76.4%   Test:  96.3% \n",
            "Iteration:1000 || Cross Entropy loss- Train: 0.078 Test: 0.099 || Batch Score- Train: 100.0% Test: 100.0% || Cumulative Score  67.5% || Character_accuracy - Train(cumulative):  76.8%   Test:  97.4% \n",
            "Iteration:1020 || Cross Entropy loss- Train: 0.092 Test: 0.066 || Batch Score- Train: 100.0% Test: 100.0% || Cumulative Score  68.2% || Character_accuracy - Train(cumulative):  77.2%   Test:  97.9% \n",
            "Iteration:1040 || Cross Entropy loss- Train: 0.091 Test: 0.089 || Batch Score- Train: 100.0% Test: 100.0% || Cumulative Score  68.8% || Character_accuracy - Train(cumulative):  77.6%   Test:  97.1% \n",
            "Iteration:1060 || Cross Entropy loss- Train: 0.056 Test: 0.079 || Batch Score- Train: 100.0% Test: 100.0% || Cumulative Score  69.4% || Character_accuracy - Train(cumulative):  78.0%   Test:  97.6% \n",
            "Iteration:1080 || Cross Entropy loss- Train: 0.064 Test: 0.172 || Batch Score- Train: 100.0% Test:  93.8% || Cumulative Score  69.9% || Character_accuracy - Train(cumulative):  78.3%   Test:  94.7% \n",
            "Iteration:1100 || Cross Entropy loss- Train: 0.088 Test: 0.081 || Batch Score- Train: 100.0% Test: 100.0% || Cumulative Score  70.4% || Character_accuracy - Train(cumulative):  78.7%   Test:  96.9% \n",
            "Iteration:1120 || Cross Entropy loss- Train: 0.053 Test: 0.059 || Batch Score- Train: 100.0% Test: 100.0% || Cumulative Score  71.0% || Character_accuracy - Train(cumulative):  79.0%   Test:  98.1% \n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Stopping Condition Achieved: Average of Last 16 (2*8) Cross entropy Loss is less than 0.09:  tensor(0.0882, grad_fn=<DivBackward0>)\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "0.9699623491109692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dkHmXDdQo4k",
        "colab_type": "text"
      },
      "source": [
        "## M - Testing on 16384 string messages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WhD7mKUtN4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train Data loader\n",
        "testing_batch_size = 64\n",
        "test_exp = int(16384/testing_batch_size)+1\n",
        "batch_score_test = []\n",
        "\n",
        "for i in range(test_exp):\n",
        "    x,y,length,a=batch_tensor(testing_batch_size)\n",
        "    y_pred=final_model(x,length)\n",
        "    batch_score_test.append(score(*numerical_string(testing_batch_size,y_pred,a,length)))\n",
        "batch_score_test = np.array(batch_score_test)\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGuntGihRy1V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd038b4d-02c6-42b8-9303-e1fc125e0c66"
      },
      "source": [
        "print(batch_score_test.mean(),batch_score_test.std())"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9936770428015564 0.010218349549686036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhvPKbZBOAec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81c8ee88-35fb-4f16-91ba-ef8bdd68f539"
      },
      "source": [
        "batch_score_test[np.where( batch_score_test < 0.95 )]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([], dtype=float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxdjTOXqbGrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOo6jpm4bKak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set(color_codes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r4ZblkwbKeg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "544d6532-cefa-475d-c4c1-88c6870b9dc8"
      },
      "source": [
        "sns.distplot(batch_score_test)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3d1240d048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD7CAYAAACL+TRnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCEiwP8H2/QRfdVHauAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnnEwL8Cce3k",
        "colab_type": "text"
      },
      "source": [
        "- The average of accuracies of all these batches would be Normal Distribution\n",
        "    - Parameters of this Normal distribution\n",
        "        - Mean: Average of accuracies\n",
        "        - Variance: Sample Variance/(Number of batches)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N96wa-NdMHx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "9020bc16-b0f4-4ae6-f5d7-4c4e5acabf78"
      },
      "source": [
        "import math\n",
        "mean = batch_score_test.mean()\n",
        "std = (batch_score_test.std())/math.sqrt(test_exp)\n",
        "x = np.random.normal(loc=mean,scale=std,size=100)\n",
        "sns.distplot(x)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3d14e2dd30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD7CAYAAACL+TRnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLHIw0wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAzfn4gue0-w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37898875-a2ad-42ec-98d7-ac91fda56a92"
      },
      "source": [
        "import scipy.stats as st\n",
        "100 - st.norm.cdf((0.9-mean)/std)*100"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9PhuNVGhLf1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3be4915-45d3-49df-9d9f-b71b0f2e4536"
      },
      "source": [
        "(0.9-mean)/std"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-146.96671237206928"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMc0OPPtgphi",
        "colab_type": "text"
      },
      "source": [
        "- This suggests that the current model gives an accuracy of more than 90% almost every time. \n",
        "- The one-tail hypothesis test on the average of accuracies gives (100-$\\epsilon$)% confidence (In reality it can't be 100%)\n",
        "- Here it is rounded because the precision required is beyond the capability of scipy.stats functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koKPW7Oz_BfA",
        "colab_type": "text"
      },
      "source": [
        "## N - Model visualization in Tensorboard\n",
        "- Download the log files form the [link](https://drive.google.com/open?id=17WO-mzmo754fUaYQYuDFkP-EZv_W0kKj) and run it on local Tensorboard\n",
        "- Log files and visualization are also uploaded to Git Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFQ2D_pMFOos",
        "colab_type": "text"
      },
      "source": [
        "# Appendix - Bayesian Optimization\n",
        "## Hyperparameter tuning\n",
        "Popularly there are 3 techniques to optimize hyperparameters of the model\n",
        "- **Grid Search**\n",
        "    - High computation is required, especially when the grid becomes higher-dimensional. \n",
        "    - It does not consider any improvements over any parameters to anchor the tuning. \n",
        "    - Brute force method\n",
        "- **Random Search**\n",
        "    - Many times random search gives better result\n",
        "    - However, this is too random for the conclusion and hope for the best parameters\n",
        "    - It becomes difficult when the number of hyperparameters increases\n",
        "- **Bayesian Optimization**\n",
        "    - Bayesian Optimization based on Gaussian process\n",
        "    - [Tutorial on Bayesian Optimization](https://arxiv.org/pdf/1012.2599.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPJuuNlpbvcb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "825d60c8-3b3e-4c71-e773-adc13a514076"
      },
      "source": [
        "!pip install hyperopt\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from hyperopt.pyll import scope\n",
        "from hyperopt.pyll.stochastic import sample\n",
        "import random\n",
        "import seaborn as sns\n",
        "import time"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.17.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.28.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.12.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.10.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5zB-OEubvuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_hyperopt= {\n",
        "    'hiddensize': hp.choice('hiddensize',[8,16,32,64]),\n",
        "    'nlayers': hp.choice('nlayers',[1,2,3]),\n",
        "    'batchsize': hp.choice('batchsize',[8,16,32,64]),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4G354VAbv-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hyperopt(param_space,num_eval):\n",
        "    \n",
        "    start = time.time()\n",
        "    \n",
        "    def objective_function(params):\n",
        "        final_model,optimizer,scheduler = model(params)\n",
        "        input_size = 27\n",
        "        output_size = 27\n",
        "        n_characters=27\n",
        "        hidden_size = params['hiddensize']\n",
        "        n_layers = params['nlayers']\n",
        "        batch_size = params['batchsize']\n",
        "        learning_rate=0.01\n",
        "\n",
        "\n",
        "        printing_model(final_model)\n",
        "        comment ='layers'+str(n_layers)+'_hiddensize'+str(hidden_size)+'_batchsize'+str(batch_size)\n",
        "        hyopt_score = train(final_model, 5000, batch_size,criterion, optimizer,comment)\n",
        "        return {'loss': -hyopt_score, 'status': STATUS_OK}\n",
        "\n",
        "    trials = Trials()\n",
        "    best_param = fmin(objective_function, \n",
        "                      param_space, \n",
        "                      algo=tpe.suggest, \n",
        "                      max_evals=num_eval, \n",
        "                      trials=trials,\n",
        "                      rstate= np.random.RandomState(1))\n",
        "    \n",
        "    loss = [x['result']['loss'] for x in trials.trials]\n",
        "    print(best_param)\n",
        "\n",
        "    print(\"Results\")\n",
        "    print(\"Best Character Accuracy: \", min(loss)*-1)\n",
        "    print(\"Best parameters: \", best_param)\n",
        "\n",
        "    \n",
        "    return best_param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKCFBLNvbwKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "312dd9e8-2fc6-4b97-b53f-4c110ffe0ea6"
      },
      "source": [
        "best_param = hyperopt(param_hyperopt,10)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing Model\n",
            "CharLSTM(\n",
            "  (encoder): Embedding(27, 27, padding_idx=0)\n",
            "  (lstm): LSTM(27, 8, batch_first=True)\n",
            "  (decoder): Linear(in_features=8, out_features=27, bias=True)\n",
            ")\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Number of Paramters: \n",
            "2156\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "torch.Size([27, 27])\n",
            "torch.Size([32, 27])\n",
            "torch.Size([32, 8])\n",
            "torch.Size([32])\n",
            "torch.Size([32])\n",
            "torch.Size([27, 8])\n",
            "torch.Size([27])\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Iteration:   0 || Cross Entropy loss- Train: 2.543 Test: 2.690 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):   0.0%   Test:   0.5% \n",
            "Iteration:  20 || Cross Entropy loss- Train: 2.377 Test: 2.214 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  10.7%   Test:  14.7% \n",
            "Iteration:  40 || Cross Entropy loss- Train: 2.208 Test: 2.420 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  12.0%   Test:  12.7% \n",
            "Iteration:  60 || Cross Entropy loss- Train: 1.994 Test: 2.284 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  12.3%   Test:  14.2% \n",
            "Iteration:  80 || Cross Entropy loss- Train: 2.098 Test: 2.097 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  12.5%   Test:  11.7% \n",
            "Iteration: 100 || Cross Entropy loss- Train: 1.999 Test: 1.886 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  13.0%   Test:  17.8% \n",
            "Iteration: 120 || Cross Entropy loss- Train: 1.992 Test: 1.589 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  14.0%   Test:  19.9% \n",
            "Iteration: 140 || Cross Entropy loss- Train: 2.099 Test: 1.717 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  15.0%   Test:  23.9% \n",
            "Iteration: 160 || Cross Entropy loss- Train: 1.817 Test: 1.660 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  15.9%   Test:  23.7% \n",
            "Iteration: 180 || Cross Entropy loss- Train: 1.716 Test: 1.828 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  16.6%   Test:  21.7% \n",
            "Iteration: 200 || Cross Entropy loss- Train: 1.820 Test: 1.890 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  17.2%   Test:  20.6% \n",
            "Iteration: 220 || Cross Entropy loss- Train: 1.697 Test: 1.901 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  17.6%   Test:  21.8% \n",
            "Iteration: 240 || Cross Entropy loss- Train: 1.694 Test: 1.723 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  18.1%   Test:  24.2% \n",
            "Iteration: 260 || Cross Entropy loss- Train: 1.856 Test: 1.875 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  18.5%   Test:  17.9% \n",
            "Iteration: 280 || Cross Entropy loss- Train: 1.690 Test: 1.470 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  18.9%   Test:  29.4% \n",
            "Iteration: 300 || Cross Entropy loss- Train: 1.654 Test: 1.850 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  19.3%   Test:  26.4% \n",
            "Iteration: 320 || Cross Entropy loss- Train: 1.641 Test: 1.529 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  19.7%   Test:  25.9% \n",
            "Iteration: 340 || Cross Entropy loss- Train: 1.552 Test: 1.778 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  20.0%   Test:  21.6% \n",
            "Iteration: 360 || Cross Entropy loss- Train: 1.292 Test: 1.611 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  20.3%   Test:  24.0% \n",
            "Iteration: 380 || Cross Entropy loss- Train: 1.531 Test: 1.705 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  20.6%   Test:  23.0% \n",
            "Iteration: 400 || Cross Entropy loss- Train: 1.583 Test: 1.410 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  20.9%   Test:  31.4% \n",
            "Iteration: 420 || Cross Entropy loss- Train: 1.458 Test: 1.678 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  21.1%   Test:  23.5% \n",
            "Iteration: 440 || Cross Entropy loss- Train: 1.724 Test: 1.633 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  21.4%   Test:  30.8% \n",
            "Iteration: 460 || Cross Entropy loss- Train: 1.670 Test: 1.620 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  21.6%   Test:  29.6% \n",
            "Iteration: 480 || Cross Entropy loss- Train: 1.710 Test: 1.983 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  21.8%   Test:  20.6% \n",
            "Iteration: 500 || Cross Entropy loss- Train: 1.530 Test: 1.404 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  22.1%   Test:  26.8% \n",
            "Iteration: 520 || Cross Entropy loss- Train: 1.775 Test: 1.373 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  22.4%   Test:  31.4% \n",
            "Iteration: 540 || Cross Entropy loss- Train: 1.451 Test: 1.571 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  22.6%   Test:  28.0% \n",
            "Iteration: 560 || Cross Entropy loss- Train: 1.790 Test: 1.791 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  22.8%   Test:  25.3% \n",
            "Iteration: 580 || Cross Entropy loss- Train: 1.403 Test: 1.244 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  23.1%   Test:  35.0% \n",
            "Iteration: 600 || Cross Entropy loss- Train: 1.663 Test: 1.176 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  23.3%   Test:  31.8% \n",
            "Iteration: 620 || Cross Entropy loss- Train: 1.427 Test: 1.467 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  23.5%   Test:  31.8% \n",
            "Iteration: 640 || Cross Entropy loss- Train: 1.354 Test: 1.445 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  23.7%   Test:  28.9% \n",
            "Iteration: 660 || Cross Entropy loss- Train: 1.265 Test: 1.593 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  23.8%   Test:  27.1% \n",
            "Iteration: 680 || Cross Entropy loss- Train: 1.582 Test: 1.359 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  24.0%   Test:  26.5% \n",
            "Iteration: 700 || Cross Entropy loss- Train: 1.674 Test: 1.422 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  24.2%   Test:  29.7% \n",
            "Iteration: 720 || Cross Entropy loss- Train: 1.624 Test: 1.448 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  24.4%   Test:  27.0% \n",
            "Iteration: 740 || Cross Entropy loss- Train: 1.510 Test: 1.592 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  24.6%   Test:  26.1% \n",
            "Iteration: 760 || Cross Entropy loss- Train: 1.583 Test: 1.390 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  24.7%   Test:  34.0% \n",
            "Iteration: 780 || Cross Entropy loss- Train: 1.629 Test: 1.444 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  24.9%   Test:  29.6% \n",
            "Iteration: 800 || Cross Entropy loss- Train: 1.334 Test: 1.775 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  25.0%   Test:  27.8% \n",
            "Iteration: 820 || Cross Entropy loss- Train: 1.466 Test: 1.398 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  25.2%   Test:  34.9% \n",
            "Iteration: 840 || Cross Entropy loss- Train: 1.480 Test: 1.559 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  25.3%   Test:  28.6% \n",
            "Iteration: 860 || Cross Entropy loss- Train: 1.379 Test: 1.518 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  25.4%   Test:  27.8% \n",
            "Iteration: 880 || Cross Entropy loss- Train: 1.385 Test: 1.252 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  25.6%   Test:  35.1% \n",
            "Iteration: 900 || Cross Entropy loss- Train: 1.595 Test: 1.324 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  25.7%   Test:  31.2% \n",
            "Iteration: 920 || Cross Entropy loss- Train: 1.319 Test: 1.513 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  25.8%   Test:  29.4% \n",
            "Iteration: 940 || Cross Entropy loss- Train: 1.484 Test: 1.268 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  25.9%   Test:  34.9% \n",
            "Iteration: 960 || Cross Entropy loss- Train: 1.538 Test: 1.460 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  26.0%   Test:  32.1% \n",
            "Iteration: 980 || Cross Entropy loss- Train: 1.657 Test: 1.589 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  26.1%   Test:  27.6% \n",
            "Iteration:1000 || Cross Entropy loss- Train: 1.434 Test: 1.442 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  26.2%   Test:  37.0% \n",
            "Iteration:1020 || Cross Entropy loss- Train: 1.162 Test: 1.440 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  26.3%   Test:  34.3% \n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "The model with current paramters is not converging in first 1000 iterations: \n",
            "tensor(1.4288, grad_fn=<DivBackward0>)\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Printing Model\n",
            "CharLSTM(\n",
            "  (encoder): Embedding(27, 27, padding_idx=0)\n",
            "  (lstm): LSTM(27, 16, batch_first=True)\n",
            "  (decoder): Linear(in_features=16, out_features=27, bias=True)\n",
            ")\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Number of Paramters: \n",
            "4068\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "torch.Size([27, 27])\n",
            "torch.Size([64, 27])\n",
            "torch.Size([64, 16])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "torch.Size([27, 16])\n",
            "torch.Size([27])\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Iteration:   0 || Cross Entropy loss- Train: 2.511 Test: 2.667 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):   3.4%   Test:   1.9% \n",
            "Iteration:  20 || Cross Entropy loss- Train: 2.434 Test: 2.462 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  11.9%   Test:  14.6% \n",
            "Iteration:  40 || Cross Entropy loss- Train: 2.321 Test: 2.401 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  13.1%   Test:  12.8% \n",
            "Iteration:  60 || Cross Entropy loss- Train: 2.047 Test: 2.145 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  13.6%   Test:  18.3% \n",
            "Iteration:  80 || Cross Entropy loss- Train: 2.063 Test: 1.990 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  14.9%   Test:  25.0% \n",
            "Iteration: 100 || Cross Entropy loss- Train: 1.977 Test: 1.999 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  16.3%   Test:  24.6% \n",
            "Iteration: 120 || Cross Entropy loss- Train: 1.898 Test: 1.773 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  17.3%   Test:  28.1% \n",
            "Iteration: 140 || Cross Entropy loss- Train: 1.790 Test: 1.825 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  18.1%   Test:  23.1% \n",
            "Iteration: 160 || Cross Entropy loss- Train: 1.744 Test: 1.517 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  18.9%   Test:  27.3% \n",
            "Iteration: 180 || Cross Entropy loss- Train: 1.708 Test: 1.635 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  19.5%   Test:  22.6% \n",
            "Iteration: 200 || Cross Entropy loss- Train: 1.825 Test: 1.645 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  20.3%   Test:  25.0% \n",
            "Iteration: 220 || Cross Entropy loss- Train: 1.465 Test: 1.651 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  21.0%   Test:  24.8% \n",
            "Iteration: 240 || Cross Entropy loss- Train: 1.543 Test: 1.620 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  21.6%   Test:  27.9% \n",
            "Iteration: 260 || Cross Entropy loss- Train: 1.550 Test: 1.664 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  22.2%   Test:  26.5% \n",
            "Iteration: 280 || Cross Entropy loss- Train: 1.414 Test: 1.608 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  22.7%   Test:  28.2% \n",
            "Iteration: 300 || Cross Entropy loss- Train: 1.640 Test: 1.517 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  23.1%   Test:  28.8% \n",
            "Iteration: 320 || Cross Entropy loss- Train: 1.652 Test: 1.358 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  23.6%   Test:  33.4% \n",
            "Iteration: 340 || Cross Entropy loss- Train: 1.446 Test: 1.495 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  24.1%   Test:  27.7% \n",
            "Iteration: 360 || Cross Entropy loss- Train: 1.648 Test: 1.354 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  24.5%   Test:  32.6% \n",
            "Iteration: 380 || Cross Entropy loss- Train: 1.404 Test: 1.494 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  24.8%   Test:  31.4% \n",
            "Iteration: 400 || Cross Entropy loss- Train: 1.253 Test: 1.580 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  25.2%   Test:  32.5% \n",
            "Iteration: 420 || Cross Entropy loss- Train: 1.219 Test: 1.529 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  25.5%   Test:  30.5% \n",
            "Iteration: 440 || Cross Entropy loss- Train: 1.345 Test: 1.493 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  25.8%   Test:  32.3% \n",
            "Iteration: 460 || Cross Entropy loss- Train: 1.442 Test: 1.339 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  26.2%   Test:  30.3% \n",
            "Iteration: 480 || Cross Entropy loss- Train: 1.383 Test: 1.423 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  26.4%   Test:  34.5% \n",
            "Iteration: 500 || Cross Entropy loss- Train: 1.433 Test: 1.507 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  26.7%   Test:  32.1% \n",
            "Iteration: 520 || Cross Entropy loss- Train: 1.326 Test: 1.215 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  27.0%   Test:  35.9% \n",
            "Iteration: 540 || Cross Entropy loss- Train: 1.328 Test: 1.377 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  27.2%   Test:  35.9% \n",
            "Iteration: 560 || Cross Entropy loss- Train: 1.325 Test: 1.241 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  27.4%   Test:  31.5% \n",
            "Iteration: 580 || Cross Entropy loss- Train: 1.368 Test: 1.382 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  27.6%   Test:  30.2% \n",
            "Iteration: 600 || Cross Entropy loss- Train: 1.273 Test: 1.282 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  27.8%   Test:  32.8% \n",
            "Iteration: 620 || Cross Entropy loss- Train: 1.352 Test: 1.362 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  28.0%   Test:  28.0% \n",
            "Iteration: 640 || Cross Entropy loss- Train: 1.424 Test: 1.285 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  28.3%   Test:  37.9% \n",
            "Iteration: 660 || Cross Entropy loss- Train: 1.162 Test: 1.381 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  28.5%   Test:  33.2% \n",
            "Iteration: 680 || Cross Entropy loss- Train: 1.281 Test: 1.280 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  28.7%   Test:  32.5% \n",
            "Iteration: 700 || Cross Entropy loss- Train: 1.179 Test: 1.236 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  28.9%   Test:  39.5% \n",
            "Iteration: 720 || Cross Entropy loss- Train: 1.304 Test: 1.432 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  29.1%   Test:  30.7% \n",
            "Iteration: 740 || Cross Entropy loss- Train: 1.230 Test: 1.253 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  29.3%   Test:  34.9% \n",
            "Iteration: 760 || Cross Entropy loss- Train: 1.101 Test: 1.385 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  29.5%   Test:  33.9% \n",
            "Iteration: 780 || Cross Entropy loss- Train: 1.522 Test: 1.192 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  29.6%   Test:  39.0% \n",
            "Iteration: 800 || Cross Entropy loss- Train: 1.375 Test: 1.308 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  29.8%   Test:  37.1% \n",
            "Iteration: 820 || Cross Entropy loss- Train: 1.178 Test: 1.333 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  30.0%   Test:  36.1% \n",
            "Iteration: 840 || Cross Entropy loss- Train: 1.200 Test: 1.164 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  30.1%   Test:  40.9% \n",
            "Iteration: 860 || Cross Entropy loss- Train: 1.188 Test: 1.338 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  30.3%   Test:  40.1% \n",
            "Iteration: 880 || Cross Entropy loss- Train: 1.302 Test: 1.230 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  30.5%   Test:  38.2% \n",
            "Iteration: 900 || Cross Entropy loss- Train: 1.134 Test: 1.215 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  30.6%   Test:  35.2% \n",
            "Iteration: 920 || Cross Entropy loss- Train: 1.177 Test: 1.127 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  30.8%   Test:  37.9% \n",
            "Iteration: 940 || Cross Entropy loss- Train: 1.084 Test: 1.125 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  30.9%   Test:  41.9% \n",
            "Iteration: 960 || Cross Entropy loss- Train: 1.167 Test: 1.262 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  31.1%   Test:  34.6% \n",
            "Iteration: 980 || Cross Entropy loss- Train: 1.177 Test: 1.316 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  31.3%   Test:  37.6% \n",
            "Iteration:1000 || Cross Entropy loss- Train: 1.233 Test: 1.231 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  31.4%   Test:  38.0% \n",
            "Iteration:1020 || Cross Entropy loss- Train: 1.111 Test: 1.061 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  31.6%   Test:  40.9% \n",
            "Iteration:1040 || Cross Entropy loss- Train: 1.197 Test: 1.222 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  31.8%   Test:  44.1% \n",
            "Iteration:1060 || Cross Entropy loss- Train: 1.169 Test: 1.089 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  31.9%   Test:  45.0% \n",
            "Iteration:1080 || Cross Entropy loss- Train: 1.133 Test: 1.175 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  32.1%   Test:  43.5% \n",
            "Iteration:1100 || Cross Entropy loss- Train: 1.225 Test: 1.110 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  32.3%   Test:  46.8% \n",
            "Iteration:1120 || Cross Entropy loss- Train: 1.103 Test: 1.242 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  32.4%   Test:  39.8% \n",
            "Iteration:1140 || Cross Entropy loss- Train: 1.302 Test: 1.206 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  32.6%   Test:  42.0% \n",
            "Iteration:1160 || Cross Entropy loss- Train: 1.109 Test: 1.199 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  32.8%   Test:  41.1% \n",
            "Iteration:1180 || Cross Entropy loss- Train: 1.305 Test: 1.326 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  32.9%   Test:  40.4% \n",
            "Iteration:1200 || Cross Entropy loss- Train: 1.104 Test: 1.008 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  33.1%   Test:  44.1% \n",
            "Iteration:1220 || Cross Entropy loss- Train: 1.095 Test: 1.084 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  33.3%   Test:  45.9% \n",
            "Iteration:1240 || Cross Entropy loss- Train: 1.063 Test: 1.082 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  33.5%   Test:  48.9% \n",
            "Iteration:1260 || Cross Entropy loss- Train: 1.166 Test: 1.026 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  33.6%   Test:  44.9% \n",
            "Iteration:1280 || Cross Entropy loss- Train: 1.069 Test: 1.002 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  33.8%   Test:  49.4% \n",
            "Iteration:1300 || Cross Entropy loss- Train: 1.252 Test: 1.171 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  34.0%   Test:  43.5% \n",
            "Iteration:1320 || Cross Entropy loss- Train: 1.055 Test: 1.194 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  34.1%   Test:  39.9% \n",
            "Iteration:1340 || Cross Entropy loss- Train: 1.049 Test: 1.274 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  34.3%   Test:  43.1% \n",
            "Iteration:1360 || Cross Entropy loss- Train: 1.191 Test: 1.181 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  34.4%   Test:  44.1% \n",
            "Iteration:1380 || Cross Entropy loss- Train: 1.093 Test: 1.092 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  34.6%   Test:  44.7% \n",
            "Iteration:1400 || Cross Entropy loss- Train: 1.095 Test: 1.097 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.0% || Character_accuracy - Train(cumulative):  34.7%   Test:  44.2% \n",
            "Iteration:1420 || Cross Entropy loss- Train: 1.108 Test: 0.991 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  34.9%   Test:  45.4% \n",
            "Iteration:1440 || Cross Entropy loss- Train: 1.032 Test: 1.065 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  35.0%   Test:  49.3% \n",
            "Iteration:1460 || Cross Entropy loss- Train: 1.102 Test: 1.193 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  35.2%   Test:  41.8% \n",
            "Iteration:1480 || Cross Entropy loss- Train: 1.169 Test: 1.028 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  35.3%   Test:  48.7% \n",
            "Iteration:1500 || Cross Entropy loss- Train: 1.076 Test: 1.241 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  35.5%   Test:  44.7% \n",
            "Iteration:1520 || Cross Entropy loss- Train: 1.134 Test: 1.034 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  35.6%   Test:  45.3% \n",
            "Iteration:1540 || Cross Entropy loss- Train: 0.916 Test: 1.136 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  35.7%   Test:  48.1% \n",
            "Iteration:1560 || Cross Entropy loss- Train: 1.030 Test: 1.005 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  35.9%   Test:  46.0% \n",
            "Iteration:1580 || Cross Entropy loss- Train: 1.202 Test: 1.099 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  36.0%   Test:  53.0% \n",
            "Iteration:1600 || Cross Entropy loss- Train: 1.168 Test: 1.152 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  36.2%   Test:  44.4% \n",
            "Iteration:1620 || Cross Entropy loss- Train: 1.080 Test: 1.041 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  36.3%   Test:  47.2% \n",
            "Iteration:1640 || Cross Entropy loss- Train: 0.917 Test: 1.111 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  36.5%   Test:  41.0% \n",
            "Iteration:1660 || Cross Entropy loss- Train: 0.926 Test: 0.975 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  36.6%   Test:  51.4% \n",
            "Iteration:1680 || Cross Entropy loss- Train: 1.156 Test: 1.059 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  36.8%   Test:  48.1% \n",
            "Iteration:1700 || Cross Entropy loss- Train: 1.148 Test: 1.045 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  36.9%   Test:  50.1% \n",
            "Iteration:1720 || Cross Entropy loss- Train: 0.893 Test: 1.078 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  37.0%   Test:  49.4% \n",
            "Iteration:1740 || Cross Entropy loss- Train: 1.026 Test: 1.093 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  37.2%   Test:  47.3% \n",
            "Iteration:1760 || Cross Entropy loss- Train: 0.982 Test: 1.065 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  37.3%   Test:  44.4% \n",
            "Iteration:1780 || Cross Entropy loss- Train: 1.124 Test: 1.167 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  37.4%   Test:  46.0% \n",
            "Iteration:1800 || Cross Entropy loss- Train: 0.919 Test: 1.131 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  37.6%   Test:  47.0% \n",
            "Iteration:1820 || Cross Entropy loss- Train: 1.089 Test: 1.006 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  37.7%   Test:  49.1% \n",
            "Iteration:1840 || Cross Entropy loss- Train: 1.107 Test: 1.095 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  37.8%   Test:  48.3% \n",
            "Iteration:1860 || Cross Entropy loss- Train: 0.904 Test: 0.998 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  38.0%   Test:  46.2% \n",
            "Iteration:1880 || Cross Entropy loss- Train: 1.037 Test: 1.040 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   0.1% || Character_accuracy - Train(cumulative):  38.1%   Test:  47.5% \n",
            "Iteration:1900 || Cross Entropy loss- Train: 0.933 Test: 1.190 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.2% || Character_accuracy - Train(cumulative):  38.2%   Test:  47.5% \n",
            "Iteration:1920 || Cross Entropy loss- Train: 0.928 Test: 1.014 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.2% || Character_accuracy - Train(cumulative):  38.3%   Test:  47.7% \n",
            "Iteration:1940 || Cross Entropy loss- Train: 0.997 Test: 0.985 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   0.2% || Character_accuracy - Train(cumulative):  38.5%   Test:  51.0% \n",
            "Iteration:1960 || Cross Entropy loss- Train: 0.960 Test: 1.064 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.2% || Character_accuracy - Train(cumulative):  38.6%   Test:  46.5% \n",
            "Iteration:1980 || Cross Entropy loss- Train: 1.148 Test: 1.075 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.2% || Character_accuracy - Train(cumulative):  38.7%   Test:  48.6% \n",
            "Iteration:2000 || Cross Entropy loss- Train: 0.820 Test: 1.053 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.2% || Character_accuracy - Train(cumulative):  38.8%   Test:  46.6% \n",
            "Iteration:2020 || Cross Entropy loss- Train: 0.965 Test: 0.903 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.2% || Character_accuracy - Train(cumulative):  38.9%   Test:  53.0% \n",
            "Iteration:2040 || Cross Entropy loss- Train: 1.176 Test: 1.001 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.2% || Character_accuracy - Train(cumulative):  39.1%   Test:  50.6% \n",
            "Iteration:2060 || Cross Entropy loss- Train: 1.049 Test: 0.716 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.2% || Character_accuracy - Train(cumulative):  39.2%   Test:  58.9% \n",
            "Iteration:2080 || Cross Entropy loss- Train: 0.908 Test: 1.084 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.2% || Character_accuracy - Train(cumulative):  39.3%   Test:  50.5% \n",
            "Iteration:2100 || Cross Entropy loss- Train: 1.049 Test: 0.994 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.2% || Character_accuracy - Train(cumulative):  39.4%   Test:  54.0% \n",
            "Iteration:2120 || Cross Entropy loss- Train: 1.091 Test: 0.993 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.3% || Character_accuracy - Train(cumulative):  39.5%   Test:  53.8% \n",
            "Iteration:2140 || Cross Entropy loss- Train: 1.029 Test: 0.986 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   0.3% || Character_accuracy - Train(cumulative):  39.6%   Test:  51.8% \n",
            "Iteration:2160 || Cross Entropy loss- Train: 1.009 Test: 1.114 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.3% || Character_accuracy - Train(cumulative):  39.7%   Test:  47.7% \n",
            "Iteration:2180 || Cross Entropy loss- Train: 0.992 Test: 0.940 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.3% || Character_accuracy - Train(cumulative):  39.8%   Test:  51.8% \n",
            "Iteration:2200 || Cross Entropy loss- Train: 1.150 Test: 1.068 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.3% || Character_accuracy - Train(cumulative):  40.0%   Test:  50.2% \n",
            "Iteration:2220 || Cross Entropy loss- Train: 1.121 Test: 1.061 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.3% || Character_accuracy - Train(cumulative):  40.1%   Test:  52.4% \n",
            "Iteration:2240 || Cross Entropy loss- Train: 0.777 Test: 0.857 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.3% || Character_accuracy - Train(cumulative):  40.2%   Test:  55.8% \n",
            "Iteration:2260 || Cross Entropy loss- Train: 0.960 Test: 0.931 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.4% || Character_accuracy - Train(cumulative):  40.3%   Test:  52.6% \n",
            "Iteration:2280 || Cross Entropy loss- Train: 0.888 Test: 1.113 || Batch Score- Train:  18.8% Test:   0.0% || Cumulative Score   0.4% || Character_accuracy - Train(cumulative):  40.4%   Test:  51.2% \n",
            "Iteration:2300 || Cross Entropy loss- Train: 1.037 Test: 1.000 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.4% || Character_accuracy - Train(cumulative):  40.5%   Test:  50.2% \n",
            "Iteration:2320 || Cross Entropy loss- Train: 0.929 Test: 0.921 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.4% || Character_accuracy - Train(cumulative):  40.6%   Test:  52.8% \n",
            "Iteration:2340 || Cross Entropy loss- Train: 1.031 Test: 0.933 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.4% || Character_accuracy - Train(cumulative):  40.7%   Test:  54.2% \n",
            "Iteration:2360 || Cross Entropy loss- Train: 0.844 Test: 1.125 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   0.4% || Character_accuracy - Train(cumulative):  40.8%   Test:  52.2% \n",
            "Iteration:2380 || Cross Entropy loss- Train: 0.961 Test: 0.876 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   0.5% || Character_accuracy - Train(cumulative):  40.9%   Test:  54.5% \n",
            "Iteration:2400 || Cross Entropy loss- Train: 1.020 Test: 0.997 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   0.5% || Character_accuracy - Train(cumulative):  41.0%   Test:  52.1% \n",
            "Iteration:2420 || Cross Entropy loss- Train: 1.011 Test: 0.867 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.5% || Character_accuracy - Train(cumulative):  41.1%   Test:  54.8% \n",
            "Iteration:2440 || Cross Entropy loss- Train: 0.950 Test: 0.953 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.5% || Character_accuracy - Train(cumulative):  41.2%   Test:  52.6% \n",
            "Iteration:2460 || Cross Entropy loss- Train: 1.075 Test: 0.881 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   0.5% || Character_accuracy - Train(cumulative):  41.3%   Test:  54.1% \n",
            "Iteration:2480 || Cross Entropy loss- Train: 0.844 Test: 0.949 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   0.5% || Character_accuracy - Train(cumulative):  41.4%   Test:  52.0% \n",
            "Iteration:2500 || Cross Entropy loss- Train: 0.956 Test: 0.907 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.6% || Character_accuracy - Train(cumulative):  41.6%   Test:  56.8% \n",
            "Iteration:2520 || Cross Entropy loss- Train: 0.883 Test: 0.971 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   0.6% || Character_accuracy - Train(cumulative):  41.7%   Test:  51.8% \n",
            "Iteration:2540 || Cross Entropy loss- Train: 1.151 Test: 0.999 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   0.6% || Character_accuracy - Train(cumulative):  41.8%   Test:  53.4% \n",
            "Iteration:2560 || Cross Entropy loss- Train: 0.918 Test: 0.910 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   0.6% || Character_accuracy - Train(cumulative):  41.9%   Test:  54.8% \n",
            "Iteration:2580 || Cross Entropy loss- Train: 0.777 Test: 0.879 || Batch Score- Train:   6.2% Test:  12.5% || Cumulative Score   0.6% || Character_accuracy - Train(cumulative):  42.0%   Test:  53.0% \n",
            "Iteration:2600 || Cross Entropy loss- Train: 0.814 Test: 1.008 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.6% || Character_accuracy - Train(cumulative):  42.1%   Test:  50.8% \n",
            "Iteration:2620 || Cross Entropy loss- Train: 0.888 Test: 0.966 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   0.7% || Character_accuracy - Train(cumulative):  42.1%   Test:  55.2% \n",
            "Iteration:2640 || Cross Entropy loss- Train: 1.001 Test: 1.126 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.7% || Character_accuracy - Train(cumulative):  42.2%   Test:  52.1% \n",
            "Iteration:2660 || Cross Entropy loss- Train: 0.936 Test: 0.761 || Batch Score- Train:  12.5% Test:   6.2% || Cumulative Score   0.7% || Character_accuracy - Train(cumulative):  42.3%   Test:  55.1% \n",
            "Iteration:2680 || Cross Entropy loss- Train: 0.891 Test: 0.947 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.7% || Character_accuracy - Train(cumulative):  42.4%   Test:  52.3% \n",
            "Iteration:2700 || Cross Entropy loss- Train: 0.925 Test: 0.863 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   0.7% || Character_accuracy - Train(cumulative):  42.5%   Test:  52.6% \n",
            "Iteration:2720 || Cross Entropy loss- Train: 1.064 Test: 0.951 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   0.8% || Character_accuracy - Train(cumulative):  42.6%   Test:  55.9% \n",
            "Iteration:2740 || Cross Entropy loss- Train: 0.937 Test: 0.802 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   0.8% || Character_accuracy - Train(cumulative):  42.7%   Test:  59.6% \n",
            "Iteration:2760 || Cross Entropy loss- Train: 0.951 Test: 0.851 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   0.8% || Character_accuracy - Train(cumulative):  42.8%   Test:  56.6% \n",
            "Iteration:2780 || Cross Entropy loss- Train: 0.944 Test: 0.923 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.8% || Character_accuracy - Train(cumulative):  42.9%   Test:  54.2% \n",
            "Iteration:2800 || Cross Entropy loss- Train: 0.885 Test: 0.911 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   0.8% || Character_accuracy - Train(cumulative):  42.9%   Test:  52.9% \n",
            "Iteration:2820 || Cross Entropy loss- Train: 0.870 Test: 0.816 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   0.8% || Character_accuracy - Train(cumulative):  43.0%   Test:  57.5% \n",
            "Iteration:2840 || Cross Entropy loss- Train: 0.783 Test: 0.822 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   0.9% || Character_accuracy - Train(cumulative):  43.1%   Test:  58.6% \n",
            "Iteration:2860 || Cross Entropy loss- Train: 0.799 Test: 0.996 || Batch Score- Train:  18.8% Test:   0.0% || Cumulative Score   0.9% || Character_accuracy - Train(cumulative):  43.2%   Test:  53.4% \n",
            "Iteration:2880 || Cross Entropy loss- Train: 0.844 Test: 0.767 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   0.9% || Character_accuracy - Train(cumulative):  43.3%   Test:  57.1% \n",
            "Iteration:2900 || Cross Entropy loss- Train: 0.965 Test: 0.843 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   0.9% || Character_accuracy - Train(cumulative):  43.4%   Test:  61.2% \n",
            "Iteration:2920 || Cross Entropy loss- Train: 0.935 Test: 0.853 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   0.9% || Character_accuracy - Train(cumulative):  43.5%   Test:  57.5% \n",
            "Iteration:2940 || Cross Entropy loss- Train: 0.754 Test: 0.645 || Batch Score- Train:   6.2% Test:  18.8% || Cumulative Score   1.0% || Character_accuracy - Train(cumulative):  43.6%   Test:  63.8% \n",
            "Iteration:2960 || Cross Entropy loss- Train: 0.747 Test: 0.964 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   1.0% || Character_accuracy - Train(cumulative):  43.6%   Test:  49.2% \n",
            "Iteration:2980 || Cross Entropy loss- Train: 1.066 Test: 0.919 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   1.0% || Character_accuracy - Train(cumulative):  43.7%   Test:  54.6% \n",
            "Iteration:3000 || Cross Entropy loss- Train: 0.827 Test: 0.964 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   1.0% || Character_accuracy - Train(cumulative):  43.8%   Test:  55.6% \n",
            "Iteration:3020 || Cross Entropy loss- Train: 0.900 Test: 0.875 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   1.0% || Character_accuracy - Train(cumulative):  43.9%   Test:  55.1% \n",
            "Iteration:3040 || Cross Entropy loss- Train: 0.783 Test: 0.792 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   1.1% || Character_accuracy - Train(cumulative):  44.0%   Test:  57.1% \n",
            "Iteration:3060 || Cross Entropy loss- Train: 0.758 Test: 0.941 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   1.1% || Character_accuracy - Train(cumulative):  44.1%   Test:  54.1% \n",
            "Iteration:3080 || Cross Entropy loss- Train: 0.848 Test: 0.959 || Batch Score- Train:  12.5% Test:   6.2% || Cumulative Score   1.1% || Character_accuracy - Train(cumulative):  44.2%   Test:  53.3% \n",
            "Iteration:3100 || Cross Entropy loss- Train: 0.879 Test: 0.969 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   1.1% || Character_accuracy - Train(cumulative):  44.2%   Test:  56.3% \n",
            "Iteration:3120 || Cross Entropy loss- Train: 0.892 Test: 0.855 || Batch Score- Train:  18.8% Test:   0.0% || Cumulative Score   1.1% || Character_accuracy - Train(cumulative):  44.3%   Test:  60.5% \n",
            "Iteration:3140 || Cross Entropy loss- Train: 0.600 Test: 0.917 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   1.2% || Character_accuracy - Train(cumulative):  44.4%   Test:  55.3% \n",
            "Iteration:3160 || Cross Entropy loss- Train: 0.859 Test: 0.920 || Batch Score- Train:  12.5% Test:   6.2% || Cumulative Score   1.2% || Character_accuracy - Train(cumulative):  44.5%   Test:  55.8% \n",
            "Iteration:3180 || Cross Entropy loss- Train: 1.038 Test: 1.005 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   1.2% || Character_accuracy - Train(cumulative):  44.6%   Test:  53.6% \n",
            "Iteration:3200 || Cross Entropy loss- Train: 0.947 Test: 0.898 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   1.2% || Character_accuracy - Train(cumulative):  44.6%   Test:  59.8% \n",
            "Iteration:3220 || Cross Entropy loss- Train: 0.762 Test: 0.912 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   1.2% || Character_accuracy - Train(cumulative):  44.7%   Test:  54.3% \n",
            "Iteration:3240 || Cross Entropy loss- Train: 0.845 Test: 0.934 || Batch Score- Train:   0.0% Test:  25.0% || Cumulative Score   1.3% || Character_accuracy - Train(cumulative):  44.8%   Test:  54.2% \n",
            "Iteration:3260 || Cross Entropy loss- Train: 0.971 Test: 0.902 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   1.3% || Character_accuracy - Train(cumulative):  44.9%   Test:  54.4% \n",
            "Iteration:3280 || Cross Entropy loss- Train: 0.841 Test: 0.654 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   1.3% || Character_accuracy - Train(cumulative):  44.9%   Test:  60.1% \n",
            "Iteration:3300 || Cross Entropy loss- Train: 0.811 Test: 0.861 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   1.3% || Character_accuracy - Train(cumulative):  45.0%   Test:  58.3% \n",
            "Iteration:3320 || Cross Entropy loss- Train: 0.771 Test: 0.780 || Batch Score- Train:   6.2% Test:  12.5% || Cumulative Score   1.3% || Character_accuracy - Train(cumulative):  45.1%   Test:  60.1% \n",
            "Iteration:3340 || Cross Entropy loss- Train: 0.757 Test: 0.744 || Batch Score- Train:   0.0% Test:  12.5% || Cumulative Score   1.3% || Character_accuracy - Train(cumulative):  45.2%   Test:  60.9% \n",
            "Iteration:3360 || Cross Entropy loss- Train: 0.761 Test: 0.938 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   1.4% || Character_accuracy - Train(cumulative):  45.2%   Test:  58.4% \n",
            "Iteration:3380 || Cross Entropy loss- Train: 0.963 Test: 1.043 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   1.4% || Character_accuracy - Train(cumulative):  45.3%   Test:  54.8% \n",
            "Iteration:3400 || Cross Entropy loss- Train: 0.883 Test: 0.841 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   1.4% || Character_accuracy - Train(cumulative):  45.4%   Test:  56.6% \n",
            "Iteration:3420 || Cross Entropy loss- Train: 0.893 Test: 0.781 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   1.4% || Character_accuracy - Train(cumulative):  45.4%   Test:  59.8% \n",
            "Iteration:3440 || Cross Entropy loss- Train: 0.903 Test: 0.802 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   1.4% || Character_accuracy - Train(cumulative):  45.5%   Test:  57.9% \n",
            "Iteration:3460 || Cross Entropy loss- Train: 0.850 Test: 0.746 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   1.4% || Character_accuracy - Train(cumulative):  45.6%   Test:  62.3% \n",
            "Iteration:3480 || Cross Entropy loss- Train: 0.864 Test: 0.610 || Batch Score- Train:  12.5% Test:  18.8% || Cumulative Score   1.4% || Character_accuracy - Train(cumulative):  45.6%   Test:  66.4% \n",
            "Iteration:3500 || Cross Entropy loss- Train: 0.873 Test: 0.859 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   1.5% || Character_accuracy - Train(cumulative):  45.7%   Test:  54.1% \n",
            "Iteration:3520 || Cross Entropy loss- Train: 0.698 Test: 0.822 || Batch Score- Train:  12.5% Test:   6.2% || Cumulative Score   1.5% || Character_accuracy - Train(cumulative):  45.8%   Test:  61.1% \n",
            "Iteration:3540 || Cross Entropy loss- Train: 0.807 Test: 0.776 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   1.5% || Character_accuracy - Train(cumulative):  45.9%   Test:  63.7% \n",
            "Iteration:3560 || Cross Entropy loss- Train: 0.881 Test: 0.754 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   1.6% || Character_accuracy - Train(cumulative):  45.9%   Test:  60.2% \n",
            "Iteration:3580 || Cross Entropy loss- Train: 0.741 Test: 0.797 || Batch Score- Train:  18.8% Test:   6.2% || Cumulative Score   1.6% || Character_accuracy - Train(cumulative):  46.0%   Test:  58.6% \n",
            "Iteration:3600 || Cross Entropy loss- Train: 0.845 Test: 0.735 || Batch Score- Train:  12.5% Test:  18.8% || Cumulative Score   1.6% || Character_accuracy - Train(cumulative):  46.0%   Test:  61.9% \n",
            "Iteration:3620 || Cross Entropy loss- Train: 0.916 Test: 0.837 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   1.6% || Character_accuracy - Train(cumulative):  46.1%   Test:  59.7% \n",
            "Iteration:3640 || Cross Entropy loss- Train: 0.813 Test: 0.875 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   1.6% || Character_accuracy - Train(cumulative):  46.2%   Test:  57.2% \n",
            "Iteration:3660 || Cross Entropy loss- Train: 0.883 Test: 1.039 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   1.7% || Character_accuracy - Train(cumulative):  46.3%   Test:  56.4% \n",
            "Iteration:3680 || Cross Entropy loss- Train: 0.979 Test: 0.773 || Batch Score- Train:   0.0% Test:  12.5% || Cumulative Score   1.7% || Character_accuracy - Train(cumulative):  46.3%   Test:  59.5% \n",
            "Iteration:3700 || Cross Entropy loss- Train: 0.744 Test: 0.562 || Batch Score- Train:  18.8% Test:  25.0% || Cumulative Score   1.7% || Character_accuracy - Train(cumulative):  46.4%   Test:  68.2% \n",
            "Iteration:3720 || Cross Entropy loss- Train: 0.951 Test: 0.839 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   1.7% || Character_accuracy - Train(cumulative):  46.5%   Test:  57.8% \n",
            "Iteration:3740 || Cross Entropy loss- Train: 0.676 Test: 0.970 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   1.7% || Character_accuracy - Train(cumulative):  46.5%   Test:  56.1% \n",
            "Iteration:3760 || Cross Entropy loss- Train: 0.836 Test: 0.845 || Batch Score- Train:   6.2% Test:  12.5% || Cumulative Score   1.8% || Character_accuracy - Train(cumulative):  46.6%   Test:  60.1% \n",
            "Iteration:3780 || Cross Entropy loss- Train: 1.025 Test: 0.827 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   1.8% || Character_accuracy - Train(cumulative):  46.6%   Test:  58.0% \n",
            "Iteration:3800 || Cross Entropy loss- Train: 0.839 Test: 0.949 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   1.8% || Character_accuracy - Train(cumulative):  46.7%   Test:  56.0% \n",
            "Iteration:3820 || Cross Entropy loss- Train: 0.888 Test: 0.757 || Batch Score- Train:  18.8% Test:  12.5% || Cumulative Score   1.8% || Character_accuracy - Train(cumulative):  46.8%   Test:  62.5% \n",
            "Iteration:3840 || Cross Entropy loss- Train: 0.941 Test: 0.821 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   1.9% || Character_accuracy - Train(cumulative):  46.8%   Test:  62.5% \n",
            "Iteration:3860 || Cross Entropy loss- Train: 0.846 Test: 0.721 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   1.9% || Character_accuracy - Train(cumulative):  46.9%   Test:  62.0% \n",
            "Iteration:3880 || Cross Entropy loss- Train: 0.911 Test: 0.934 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   1.9% || Character_accuracy - Train(cumulative):  46.9%   Test:  57.2% \n",
            "Iteration:3900 || Cross Entropy loss- Train: 0.665 Test: 0.791 || Batch Score- Train:  12.5% Test:   0.0% || Cumulative Score   1.9% || Character_accuracy - Train(cumulative):  47.0%   Test:  59.4% \n",
            "Iteration:3920 || Cross Entropy loss- Train: 0.818 Test: 0.749 || Batch Score- Train:  18.8% Test:   6.2% || Cumulative Score   2.0% || Character_accuracy - Train(cumulative):  47.1%   Test:  62.3% \n",
            "Iteration:3940 || Cross Entropy loss- Train: 0.746 Test: 0.806 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   2.0% || Character_accuracy - Train(cumulative):  47.1%   Test:  56.7% \n",
            "Iteration:3960 || Cross Entropy loss- Train: 0.808 Test: 0.822 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   2.0% || Character_accuracy - Train(cumulative):  47.2%   Test:  58.4% \n",
            "Iteration:3980 || Cross Entropy loss- Train: 0.967 Test: 0.851 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   2.0% || Character_accuracy - Train(cumulative):  47.3%   Test:  60.5% \n",
            "Iteration:4000 || Cross Entropy loss- Train: 0.803 Test: 0.905 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   2.0% || Character_accuracy - Train(cumulative):  47.3%   Test:  56.4% \n",
            "Iteration:4020 || Cross Entropy loss- Train: 0.948 Test: 0.856 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   2.1% || Character_accuracy - Train(cumulative):  47.4%   Test:  58.5% \n",
            "Iteration:4040 || Cross Entropy loss- Train: 0.874 Test: 0.918 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   2.1% || Character_accuracy - Train(cumulative):  47.4%   Test:  60.6% \n",
            "Iteration:4060 || Cross Entropy loss- Train: 0.877 Test: 0.748 || Batch Score- Train:  12.5% Test:   0.0% || Cumulative Score   2.1% || Character_accuracy - Train(cumulative):  47.5%   Test:  59.1% \n",
            "Iteration:4080 || Cross Entropy loss- Train: 0.845 Test: 1.012 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   2.1% || Character_accuracy - Train(cumulative):  47.5%   Test:  54.0% \n",
            "Iteration:4100 || Cross Entropy loss- Train: 0.737 Test: 0.827 || Batch Score- Train:  12.5% Test:   0.0% || Cumulative Score   2.1% || Character_accuracy - Train(cumulative):  47.6%   Test:  60.7% \n",
            "Iteration:4120 || Cross Entropy loss- Train: 0.812 Test: 0.724 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   2.2% || Character_accuracy - Train(cumulative):  47.6%   Test:  60.2% \n",
            "Iteration:4140 || Cross Entropy loss- Train: 0.738 Test: 0.838 || Batch Score- Train:  12.5% Test:  18.8% || Cumulative Score   2.2% || Character_accuracy - Train(cumulative):  47.7%   Test:  61.7% \n",
            "Iteration:4160 || Cross Entropy loss- Train: 0.782 Test: 0.721 || Batch Score- Train:  12.5% Test:   6.2% || Cumulative Score   2.2% || Character_accuracy - Train(cumulative):  47.8%   Test:  63.2% \n",
            "Iteration:4180 || Cross Entropy loss- Train: 0.888 Test: 0.964 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   2.2% || Character_accuracy - Train(cumulative):  47.8%   Test:  57.7% \n",
            "Iteration:4200 || Cross Entropy loss- Train: 0.842 Test: 0.815 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   2.2% || Character_accuracy - Train(cumulative):  47.9%   Test:  62.2% \n",
            "Iteration:4220 || Cross Entropy loss- Train: 0.865 Test: 0.874 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   2.3% || Character_accuracy - Train(cumulative):  47.9%   Test:  58.7% \n",
            "Iteration:4240 || Cross Entropy loss- Train: 0.899 Test: 0.905 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   2.3% || Character_accuracy - Train(cumulative):  48.0%   Test:  60.2% \n",
            "Iteration:4260 || Cross Entropy loss- Train: 0.723 Test: 0.935 || Batch Score- Train:  18.8% Test:   6.2% || Cumulative Score   2.3% || Character_accuracy - Train(cumulative):  48.0%   Test:  55.2% \n",
            "Iteration:4280 || Cross Entropy loss- Train: 0.903 Test: 0.770 || Batch Score- Train:  12.5% Test:   6.2% || Cumulative Score   2.4% || Character_accuracy - Train(cumulative):  48.1%   Test:  64.6% \n",
            "Iteration:4300 || Cross Entropy loss- Train: 0.903 Test: 0.897 || Batch Score- Train:   0.0% Test:  12.5% || Cumulative Score   2.4% || Character_accuracy - Train(cumulative):  48.2%   Test:  62.1% \n",
            "Iteration:4320 || Cross Entropy loss- Train: 0.671 Test: 0.652 || Batch Score- Train:  12.5% Test:  18.8% || Cumulative Score   2.4% || Character_accuracy - Train(cumulative):  48.2%   Test:  64.9% \n",
            "Iteration:4340 || Cross Entropy loss- Train: 0.904 Test: 0.900 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   2.4% || Character_accuracy - Train(cumulative):  48.3%   Test:  55.9% \n",
            "Iteration:4360 || Cross Entropy loss- Train: 0.880 Test: 0.835 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   2.4% || Character_accuracy - Train(cumulative):  48.3%   Test:  59.2% \n",
            "Iteration:4380 || Cross Entropy loss- Train: 0.779 Test: 0.789 || Batch Score- Train:  12.5% Test:   6.2% || Cumulative Score   2.4% || Character_accuracy - Train(cumulative):  48.4%   Test:  61.6% \n",
            "Iteration:4400 || Cross Entropy loss- Train: 0.741 Test: 0.829 || Batch Score- Train:  25.0% Test:  12.5% || Cumulative Score   2.5% || Character_accuracy - Train(cumulative):  48.4%   Test:  60.3% \n",
            "Iteration:4420 || Cross Entropy loss- Train: 0.798 Test: 0.958 || Batch Score- Train:  12.5% Test:   6.2% || Cumulative Score   2.5% || Character_accuracy - Train(cumulative):  48.5%   Test:  57.8% \n",
            "Iteration:4440 || Cross Entropy loss- Train: 0.859 Test: 0.753 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   2.5% || Character_accuracy - Train(cumulative):  48.5%   Test:  61.6% \n",
            "Iteration:4460 || Cross Entropy loss- Train: 0.727 Test: 0.791 || Batch Score- Train:  12.5% Test:  12.5% || Cumulative Score   2.6% || Character_accuracy - Train(cumulative):  48.6%   Test:  60.4% \n",
            "Iteration:4480 || Cross Entropy loss- Train: 0.894 Test: 0.785 || Batch Score- Train:  12.5% Test:   6.2% || Cumulative Score   2.6% || Character_accuracy - Train(cumulative):  48.6%   Test:  59.9% \n",
            "Iteration:4500 || Cross Entropy loss- Train: 0.907 Test: 0.794 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   2.6% || Character_accuracy - Train(cumulative):  48.7%   Test:  61.1% \n",
            "Iteration:4520 || Cross Entropy loss- Train: 0.901 Test: 0.892 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   2.6% || Character_accuracy - Train(cumulative):  48.7%   Test:  59.2% \n",
            "Iteration:4540 || Cross Entropy loss- Train: 0.939 Test: 0.727 || Batch Score- Train:  18.8% Test:   6.2% || Cumulative Score   2.7% || Character_accuracy - Train(cumulative):  48.8%   Test:  63.8% \n",
            "Iteration:4560 || Cross Entropy loss- Train: 0.968 Test: 0.656 || Batch Score- Train:  12.5% Test:   6.2% || Cumulative Score   2.7% || Character_accuracy - Train(cumulative):  48.8%   Test:  66.2% \n",
            "Iteration:4580 || Cross Entropy loss- Train: 0.638 Test: 0.843 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   2.7% || Character_accuracy - Train(cumulative):  48.9%   Test:  59.2% \n",
            "Iteration:4600 || Cross Entropy loss- Train: 0.772 Test: 0.920 || Batch Score- Train:   0.0% Test:  12.5% || Cumulative Score   2.7% || Character_accuracy - Train(cumulative):  48.9%   Test:  57.2% \n",
            "Iteration:4620 || Cross Entropy loss- Train: 0.885 Test: 0.987 || Batch Score- Train:   0.0% Test:   0.0% || Cumulative Score   2.7% || Character_accuracy - Train(cumulative):  49.0%   Test:  55.0% \n",
            "Iteration:4640 || Cross Entropy loss- Train: 0.781 Test: 1.004 || Batch Score- Train:   6.2% Test:   0.0% || Cumulative Score   2.8% || Character_accuracy - Train(cumulative):  49.0%   Test:  56.6% \n",
            "Iteration:4660 || Cross Entropy loss- Train: 0.791 Test: 0.695 || Batch Score- Train:  18.8% Test:  18.8% || Cumulative Score   2.8% || Character_accuracy - Train(cumulative):  49.1%   Test:  65.6% \n",
            "Iteration:4680 || Cross Entropy loss- Train: 0.715 Test: 0.703 || Batch Score- Train:  12.5% Test:  12.5% || Cumulative Score   2.8% || Character_accuracy - Train(cumulative):  49.1%   Test:  62.7% \n",
            "Iteration:4700 || Cross Entropy loss- Train: 0.841 Test: 0.803 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   2.8% || Character_accuracy - Train(cumulative):  49.2%   Test:  61.2% \n",
            "Iteration:4720 || Cross Entropy loss- Train: 0.873 Test: 0.731 || Batch Score- Train:   6.2% Test:  12.5% || Cumulative Score   2.9% || Character_accuracy - Train(cumulative):  49.2%   Test:  66.3% \n",
            "Iteration:4740 || Cross Entropy loss- Train: 0.816 Test: 0.769 || Batch Score- Train:  12.5% Test:   6.2% || Cumulative Score   2.9% || Character_accuracy - Train(cumulative):  49.3%   Test:  61.5% \n",
            "Iteration:4760 || Cross Entropy loss- Train: 0.857 Test: 0.922 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   2.9% || Character_accuracy - Train(cumulative):  49.3%   Test:  59.0% \n",
            "Iteration:4780 || Cross Entropy loss- Train: 0.793 Test: 0.867 || Batch Score- Train:   0.0% Test:   6.2% || Cumulative Score   2.9% || Character_accuracy - Train(cumulative):  49.4%   Test:  57.4% \n",
            "Iteration:4800 || Cross Entropy loss- Train: 0.718 Test: 0.741 || Batch Score- Train:  12.5% Test:  12.5% || Cumulative Score   2.9% || Character_accuracy - Train(cumulative):  49.4%   Test:  63.3% \n",
            "Iteration:4820 || Cross Entropy loss- Train: 0.730 Test: 0.731 || Batch Score- Train:  12.5% Test:  18.8% || Cumulative Score   3.0% || Character_accuracy - Train(cumulative):  49.5%   Test:  60.7% \n",
            "Iteration:4840 || Cross Entropy loss- Train: 0.621 Test: 0.777 || Batch Score- Train:  18.8% Test:   0.0% || Cumulative Score   3.0% || Character_accuracy - Train(cumulative):  49.5%   Test:  63.4% \n",
            "Iteration:4860 || Cross Entropy loss- Train: 0.891 Test: 0.654 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   3.0% || Character_accuracy - Train(cumulative):  49.6%   Test:  63.9% \n",
            "Iteration:4880 || Cross Entropy loss- Train: 0.885 Test: 0.875 || Batch Score- Train:   0.0% Test:  12.5% || Cumulative Score   3.1% || Character_accuracy - Train(cumulative):  49.6%   Test:  62.0% \n",
            "Iteration:4900 || Cross Entropy loss- Train: 0.805 Test: 0.709 || Batch Score- Train:   6.2% Test:  12.5% || Cumulative Score   3.1% || Character_accuracy - Train(cumulative):  49.7%   Test:  62.2% \n",
            "Iteration:4920 || Cross Entropy loss- Train: 0.680 Test: 0.773 || Batch Score- Train:  37.5% Test:   6.2% || Cumulative Score   3.1% || Character_accuracy - Train(cumulative):  49.7%   Test:  59.1% \n",
            "Iteration:4940 || Cross Entropy loss- Train: 0.795 Test: 0.811 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   3.1% || Character_accuracy - Train(cumulative):  49.8%   Test:  62.2% \n",
            "Iteration:4960 || Cross Entropy loss- Train: 0.918 Test: 0.879 || Batch Score- Train:   6.2% Test:   6.2% || Cumulative Score   3.1% || Character_accuracy - Train(cumulative):  49.8%   Test:  60.5% \n",
            "Iteration:4980 || Cross Entropy loss- Train: 0.764 Test: 0.805 || Batch Score- Train:   0.0% Test:  12.5% || Cumulative Score   3.2% || Character_accuracy - Train(cumulative):  49.9%   Test:  62.9% \n",
            " 10%|█         | 1/10 [15:44<21:59, 146.63s/it, best loss: -0.32717342533398797]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-47ba97fe39dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_hyperopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-66-631ae1ab2561>\u001b[0m in \u001b[0;36mhyperopt\u001b[0;34m(param_space, num_eval)\u001b[0m\n\u001b[1;32m     25\u001b[0m                       \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                       \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                       rstate= np.random.RandomState(1))\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-631ae1ab2561>\u001b[0m in \u001b[0;36mobjective_function\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcomment\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_hiddensize'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_batchsize'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mhyopt_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mhyopt_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'status'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'NoneType'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FJdjNJDUQ_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}