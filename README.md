# Scale AI Engima_machine coding assignment
Learning the Enigma with LSTM
## Problem Statement ##

Implement a deep sequence model that can decrypt messages from an enigma machine. The messages can be up to 42 characters in length. Do not use an unnecessarily large network, anything more than 60K parameter is overkill.
You are expected to achieve a score of 0.9, feel free to predict longer sequences as a stretch goal.

## Contents of the repo ##

1. Readme file
2. Helper Functions provided [cipher_take_home_py]()
3. Jupyter Notebook - Overall | [Google Collab Notebook](https://colab.research.google.com/drive/1uYrSfZqJTLRHxzXmY-3TBrpzX2ww3_GY) | [main_file.ipynb]()
4. Jupyter Notebook - Only testing by load saved model | [Google Collab Notebook](https://colab.research.google.com/drive/10wDGdFqsf93PONiIFHibaZUFzbxIQfHh) | [test_file.ipynb]()


## How to run the files and get the results ##

1. Overall File
    * Run the Python notebook on Google Collab. Particular instructions are mentioned in notebook
2. Test File
    * Run the Python notebook on Google Collab. Particular instructions are mentioned in notebook

## Data Engineering ##

- The decryption of the first letter remains same for a particular varibale.
- Therefore, there is no need to learn the key for the message becuase the Enigma machine is already set to a particular configuration
- The data generated does not need any data cleaning and pre-preocessing since it is perfect simulated data

## Model ##

1)   *(done)*
2)  *(addressed)*
